{
    "contents" : "---\ntitle: \"Intro_Statistical_Learning\"\nauthor: \"Zach F.\"\ndate: \"March 16, 2015\"\noutput:\n  html_document:\n    number_sections: yes\n    smart: no\n    toc: yes\n---\n> The first part of this Rmd will be notes and exercise from the lecture series on the book Intro to Statistical Learning. The Second part of this Rmd document will contain notes and examples from the Book itself.\n***   \n\n#Chapter 1 Introduction \n***\n##Opening Remarks\n***\n###The Supervised Learning Problem    \n*Starting Point*   \n\n* Outcome measurement $Y$ (also called dependent variable, reponse,target).    \n* Vector of $p$ predictor measurement $X$ (also called inputs, regressors, covariates, features, independent variables).     \n* In the *regression problems*, $Y$ is quantitative (e.g. price, blood pressure).     \n* In the *Classification problem*, $Y$ takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample).         \n* We have training data $(x_{1},y{1}),...,(x_{N},y_{N})$. These are observations  (examples, instances) of these measurements.    \n\n###Objectives \nOn the basis of the training data we would like to:  \n* Accurately predict unseen test cases.    \n* Understand which inputs affect the outcome, and how.   \n* Assess the quality of our predictions and inferences.    \n\n##Examples and Frameworks\n***\n###Unsupervised Learning \n* No outcome variablem just a set of predictors (featurs)\nmeasured on a set of samples.    \n* objective is more fuzzy -- find groups of samples that behave similarly, find features that beghave similarly, find linear combinations of features witht the most variation.    \n* difficult to know how well your doing.    \n* different from supervised learning, but can be useful as a pre-processing step for supervised learning.    \n\n###Statistical Learning versus Machine Learning \n* Machine learning arose as a subfield of Artificial Intelligence   \n* Statistical learning arose as a subfield of Statistics    \n* *There is much overlap* -- both fields focus on supervised and unsupervised problems:   \n    * Machine learning has a greater emphasis on *large scale* applications and *prediction accuracy*.    \n    * Statistical learning emphasizes *models* and their interpretability, and *precision* and *uncertainty*.    \n* But the distinction has become more and more blurred, and there is a great deal of \"cross-fertilization\".   \n* Machine learning has the upper hand in *Marketing!*   \n\n#Chapter 2 Overview of Statistical Learning\n***\n##Introduction to Regression Models\n***   \nWhat is Statistical Learning?      \n\n> expected value is a fancy word for the average value \n\nExample of statistical learning: \nCan we predict **Sales** using these three variables?     \n\n$$ Sales \\approx f(TV,Radio,Newspaper) $$   \n\n###Notation  \n\nHere **Sales** is a *response* or *target* that we wish to predict. We generically refer to the response as $Y$. **TV** is a *feature*, or *input*, or *predictor*, we name it $X_{1}$. Likewise name **Radio** as $X_{2}$, and so on. We can refer to the *input vector* collectively as:    \n$$ X = \\begin{pmatrix} X_{1} \\\\ X_{2} \\\\ X_{3} \\end{pmatrix} $$ \nNow we write our model as:  \n$$ Y = f(X) + \\epsilon $$ \n\nwhere $\\epsilon$ captures measurements errors and other discrepancies.  \n\n###What is $f(X)$ good for?   \n\n* With a good $f$ we can make predictions of $Y$ at new points $X=x$.   \n* We can understand which components of $X=(X_{1},X_{2},...,X_{p})$ are important in explaining $Y$, and which are irrelevant. e.g. **Seniority** and **Years of Education** have a big impact on **Income**, but **Marital Status** typically does not.   \n* Depending on the complexity of $f$, we may be able to understand how each component $X_{j}$ of $X$ affects $Y$.  \n\nIs there an ideal $f(X)$? In particular, what is a good value for $f(X)$ at any selectd value of $X$, say $X$ = 4? There can be many $Y$ values at $X$ = 4. A good value is:  \n$$ f(4) = E(Y\\mid X = 4) $$   \n$E(Y\\mid X = 4)$ means *expected value* (average) of $Y$ given $X$ =4.   \nThis ideal $f(x) = E(Y\\mid X = x)$ is called the *regression function*.   \n\n###The Regression function $f(x)$   \n* Is also defined for vector $X$; e.g.   \n$$ f(x) = f(x_{1},x_{2},x_{3}) = E(Y\\mid X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3}) $$     \n* Is the *ideal* or *optimal* predictor of $Y$ with regard to mean-squared rediction error: $f(x) = E(Y\\mid X = x)$ is the function that minimizes $E[(Y - g(X))^2 \\mid X=x]$ over all functions $g$ at all points $X=x$.   \n* $\\epsilon = Y - f(x)$ is the *irreducible* error -- i.e. even if we knew $f(X)$, we would still make errors in prediction, since at each $X=x$  there is typically a distribution of possible $Y$ values.  \n* For any estimate $\\hat f(x)$ of $f(x)$, we have:  \n$$ E[(Y - \\hat f(X))^2 \\mid X =x ] = \\underbrace{[f(x) - \\hat f(x)]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible} $$     \n> hats over functions tend to represent estimators.     \n \n###How to estimate $f$   \n* Typically we have few if any data points with $X=4$ exactly.  \n* So we cannot compute $E(Y\\mid X = x)!$   \n* Relax the definition and let:  \n$$ \\hat f(x) = Ave(Y\\mid X \\in N(x)) $$     \nwhere $N(x)$ is some *neighborhood* of $x$.    \n\n> This is called nearest neighbors or local averaging, relaxing the definition to allow for averaging of nearby points. This compensates for the fact the there may not be y point for any particular x point.  \n\n##Dimensionality and Structured Models \n*** \n* Nearest neighbor averaging can be pretty good for small $p$ -- i.e. $ p \\leq 4$ and large-ish $N$.    \n* We will discuss smoother versions, such as kernel and spline smoothing later in the course.  \n* Nearest neighbor methods can be *lousy* when $p$ is large. Reason: the *curse of dimensionality*. Nearest neighbors tend to be far away in high dimensions. \n    * We need to get reasonable fraction of the $N$ values of $y_{i}$ to average to bring the variance down, --e.g. 10%.  \n    * A 10% neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating $E(Y\\mid X=x)$ by local averageing. \n\n###Parametric and structured models  \nThe *linear* model is an important example of a parametric model:  \n$$ F_{L}(X) = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ...\\beta_{p}X_{p} $$    \n* A linear model is specified in terms of $p + 1$ parameters $\\beta_{0},\\beta_{1},...,\\beta_{p}$.    \n* We estimate the parameters by fitting the model to training data.\n* Although it is *almost never correct*, a linear model often serves as a good and interpretable approximation to the unknown true function $f(X)$.   \n\n###Some trade-offs  \n* Prediction accuracy versus interprability.  \n    * Linear models are easy to interpret; thin-plate splines are not. \n* Good fit versus over-fit or under-fit.\n    * How do we know when the fit is right?\n* Parsimony versus black-blox.\n    * We often prefer a simpler model involving fewer variable over a black-box predictor involving them all.\n    \n##Model Selection and Bias-Variance Tradeoff \n***\n###Assessing Model Accuracy  \nSuppose we fit a model $\\hat f(x)$ to some training data $Tr={x_{i},y_{i}}_{1}^N$, and we wish to see how well it performs.  \n* We could compute the average squared prediction error over Tr:     \n$$ MSE_{Tr} = Ave_{i \\in Tr}[y_{i} - \\hat f(x_{i})]^2 $$    \nThis may be biased toward more overfit models.   \n* Instead we should, if possible, compute it using fresh data $Te = {x_{i},y_{i}}_{1}^M$:     \n$$ MSE_{Te} = AVE+{i \\in Te}[y_{i} - \\hat f(x_{i})]^2 $$   \n\n###Bias-Variance Trade-off   \nSuppose we haec fit a model $\\hat f(x)$ to some trianing data **Tr**, and let $(x_{0},y_{0})$ be a test observation drawn from the population. If the true model is $Y = f(X) + \\epsilon$ ( with $f(x) = E(Y\\mid X = x)$), then     \n$$ E(y_{0} - \\hat f(x_{0}))^2 = Var(\\hat f(x_{0})) + [Bias(\\hat fx_{0}))]^2 + Var(\\epsilon)  $$  \n\nThe the expectation averages over the variability of $y_{0}$, as well as the variability in **Tr**. Note that the $Bias(\\hat f(x_{0})) = E[\\hat f(x_{0})] - f(x_{0})$.   \n\nTypically as the *flexibility* of $\\hat f$ increases, its variance increases and its bias decreases. So choosing the flexibility based on average test error emounts to a *bias-variance trade-off*.   \n\n##Classification\n***\n###Classification Problems   \nHere the response variable $Y$ is *qualitative* - e.g. email is one of $C = (spam,ham)$ (ham = good email), digit class is one of $C = {0,1,...,9}$. Our gols are to:    \n* Build a classifier $C(X)$ that assigns a class label from $C$ to a future unlabeled observation $X$.   \n*  Assess the uncertainty in each classification    \n* Understand the roles of the different predictors among $X = (X_{1},X_{2},...,X_{p})$.    \n\n###Classification: Some details   \n* Typically we measure the performance of $\\hat C(x)$ using the misclassification error rate:    \n$$ Err_{Te} = Ave_{i \\in Te}I[y_{i} \\neq \\hat c(x_{i})] $$   \n* The Bayes classifier ( using the true $p_{k}(x)$ ) has smallest error ( in the population)    \n* Support-vector machines build structured models for $C(x)$.   \n* We will also build structured models for representing the $p_{k}(x)$. e.g. Logistic regression, generalized additive models.   \n\n###Introduction to R\n***    \n```{r echo = FALSE}\n###Vectors, data, matrices, subsetting\nx = c(2,7,5)\nx\ny = seq(from = 4, length = 3, by =3)\n?seq\ny\nx + y\nx / y\nx ^ y\nx[2]\nx[2:3]\nx[-2]\nx[ -c(1,2) ]\nz = matrix(seq(1,12),4,3)\nz\nz[3:4,2:3]\nz[,2:3]\nz[,1]\nz[,1,drop=FALSE]\ndim(z)\nls()\nrm(y)\nls()\n### Generating random data, graphics   \nx = runif(50)\ny = rnorm(50)\nplot(x,y)\nplot(x,y,xlab=\"Random Uniform\", ylab=\"Random Normal\", pch=\"+\", col= \"blue\")\npar(mfrow=c(2,1))\nplot(x,y)\nhist(y)\npar(mfrow=c(1,1))\n### Reading in data\n#Auto = read.csv(\"~/Desktop/Rsessions/Auto.csv\")\n?datasets\nAuto = mtcars\nnames(Auto)\ndim(Auto)\nclass(Auto)\nsummary(Auto)\nplot(Auto$cyl,Auto$mpg)\n?par\n.pardefault <- par(no.readonly = T)\nattach(Auto)\nsearch()\nplot(cyl,mpg)\n\n\n```\n\n#Chapter 3: Linear Regression\n***   \n##Simple Linear Regression \n***\n###Linear Regression   \n* Linear regression is a simple approach to supervised learning. It assumes that the dependence of $Y$ on $X_{1},X_{2},...,X_{p}$ is linear.    \n* True regression functions are never linear!    \n* Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.   \n\n###Linear regression for the advertising data   \nConsider the advertising data shown on the next slide.   \n\nQuestions we might ask:     \n* Is there a relationship between advertising budget and sales?    \n* How strong is the relationship betweeen advertising budget and sales?    \n* Which media contribute to sales?    \n* How accurately can we predict future sales?    \n* Is the relationship linear?   \n* Is there synergy among the advertising media?    \n\n###Simple linear regression using a single predictor $X$.   \n* We assume a model    \n$$ Y = \\beta_{0} + \\beta_{x}X + \\epsilon $$,   \nWhere $\\beta_{0}$ and $\\beta_{1}$ are two unknown constants that represent the *intercept* and *slope*, also known as *coefficients* or *parameters*, and $\\epsilon$ is the error term.    \n* Given some estimates $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$ for the model coefficients, we predict the future sales using    \n$$ \\hat y = \\hat\\beta_{0} + \\hat\\beta_{1}x $$,   \nWhere $\\hat y$ indicates a prediction of $Y$ on the basis of $X = x$. The *hat* symbol denotes an estimated value.      \n\n###Estimation of the parameters by least squares   \n* let $\\hat y_{i} = \\hat\\beta_{0} + \\hat\\beta_{1}x_{i}$ be the prediction for $Y$ based on the ith value of $X$. Then $e_{i} = y_{i} - \\hat y_{i}$ represents the ith *residual*.     \n* We define the *residual sum of squares* (RSS) as   \n$$ RSS = e_{1}^2 + e_{2}^2 +...+e_{n}^2 $$,   \nor equivalently as    \n$$ RSS = (y_{1} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{1})^2 + (y_{2} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{2})^2 +...+(y_{n} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{n})^2 $$     \n* The least squares approach chooses $\\hat\\beta_{0}$ and $\\hat\\beta_{1}$ to minimize the RSS. The minimizing values can be shown to be   \n$$ \\hat\\beta_{1} = \\frac{\\sum_{i=1}^n(x_{i}-\\bar x)(y_{i}-\\bar y)}{\\sum_{i =1}^n(x_{i}-\\bar x)^2} $$,     \n$$ \\hat\\beta_{0}=\\bar y - \\hat\\beta_{1}\\bar x $$,   \nWhere $\\bar y \\equiv \\frac{1}{n}\\sum_{i =1}^n x_{i}$ are the sample means.   \n\n###Assesing the Accuracy of the Coefficient Estimates   \n* The standard error of an estimator reflects how it varies under repeated sampling. We have   \n$$ SE(\\beta_{1})^2 = \\frac{\\sigma^2}{\\sum_{i =1}^n (x_{i}-\\bar x)^2}$$,   \n$$ SE(\\hat\\beta_{0})^2 = \\sigma^2[\\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i =1}^n (x_{i} - \\bar x)^2}] $$,   \nwhere $\\sigma^2 =  Var(\\epsilon)$   \n* These Standard errors can be used to compute *confidence intervals*. A 95% confidence interval is defined as a range of values such that with 95%  probability, the range will contain the true unknown value of the parameter. It has the form   \n$$ \\hat\\beta_{1} \\pm 2 \\cdot SE(\\hat\\beta_{1}) $$    \n\nThat is, there is approximately a 95% chance that the interval   \n$$ [ \\hat\\beta_{1} -2 \\cdot SE(\\hat\\beta_{1}), \\hat\\beta_{1} + 2 \\cdot SE(\\hat\\beta_{1})]  $$   \nwill contain the true value of $\\beta_{1}$ (under a scenario where we got repeated samples like the present sample)    \n\nFor the advertising data, the 95% confindence interval for $\\beta_{1}$ is [0.042,0.05]     \n\n##Hypothesis Testing and Confidence Intervals   \n***   \nHypothesis Testing    \n* Standard errors can also be used to perform *hypthesis test* on the coefficients. The most common hypothesis test involves testing the *null hypothesis* of    \n    * $H_{0}:$ There is no relationship between $X$ and $Y$ versus the *alternative hypothesis*   \n    * $H_{A}:$ There is some relationship between $X$ and $Y$   \n\n* Mathematically, this corresponds to testing   \n    * $H_{0} : \\beta_{1} = 0$    \n    versus     \n    * $H_{A} : \\beta_{1} \\neq 0$,  \nsince if $\\beta_{1} = 0$ then the model reduces to $Y = \\beta_{0} + \\epsilon$, and $X$ is not associated with $Y$.    \n\n* To test the null hypothesis, we compute a *t-statistic*, given by   \n$$ t = \\frac{\\hat\\beta_{1} - 0}{SE(\\hat\\beta_{1})} $$  \n* This will have a t-distribution with n - 2 degrees of freedom, assuming $\\beta_{0} = 0$.   \n* Using statistical software, it is easy to compute the probability of observing any value equal to $|t|$ or larger. We call this probability the *p-value*.   \n\n###Assessing the Overall Accuracy of the Model    \n* We compute the *Residual Standard Error*   \n$$ RSE = \\sqrt{\\frac{1}{n -2}RSS} = \\sqrt{\\frac{1}{n -2}\\sum_{i=1}^n (y_{i} - \\hat y_{i})^2} $$, \nwhere the *residual sum-of-squares* is $RSS = \\sum_{i=1}^n (y_{i}-\\hat y_{i})^2$.   \n\n* *R-squared* or fraction of variance explained is  \n$$ R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} $$  \nwhere $TSS = \\sum_{i=1}^n (y_{i} - \\bar y)^2$ is the *total sum of squares*.   \n* It can be shown that in this simple linear regression setting that $R^2 = r^2$, where $r$ is the correlation between $X$ and $Y$:   \n$$ r = \\frac{\\sum_{i=1}^n (x_{i} - \\bar x)(y_{i} - \\bar y)}{\\sqrt{\\sum_{i=1}^n (x_{i} - \\bar x)^2 \\sum_{i=1}^n (y_{i} - \\bar y)^2}} $$   \n\n##Multiple Linear Regression\n***  \n* Here our model is   \n$$ Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} +...+\\beta_{p}X_{p} + \\epsilon $$  \n* We interpret $\\beta_{j}$ as the *average* effect on $Y$ of a one unit increase in $X_{j}$, *holding all other predictors fixed*. In the advertising example, the model becomes   \n\n$$ sales = \\beta_{0} + \\beta{1} \\times TV + \\beta_{2} \\times radio + \\beta_{3} \\times newspaper + \\epsilon $$   \n\n###Interpreting the regression coefficients   \n* The ideal scenario is when the predictors are uncorrelated\n    * A *balanced design*:  \n    * Each coefficient can be estimated and tested seperately. \n    * Interpretations such as *\"a unit change in $X_{j}$ is associated with a $B_{j}$ change in $Y$, while all the other variables stay fixed\"*, are possible.   \n* Correlations amongst predictors cause problems:   \n    * The variance of all coefficients tends to increase, sometimes dramatically  \n    * Interpretations become hazardous - When $X_{j}$ changes, everything else changes.   \n* *Claims of causality* should be avoided for observational data.   \n\n###The woes of (interpreting) regression coefficients  \n*\"Data Analysis and Regression\" Mosteller and Tukey 1977*    \n* a regression coefficient $B_{J}$ estimates the expected change in $Y$ per unit change in $X_{j}$, *with all other predictors held fixed.* But predictors usually change together!    \n* Example: $Y$ total amount of change in your pocket:  $X_{1}$ = # of coins; $X_{2}$ = # of pennies, nickels and dimes. By itself, regression coefficinet of $Y$ on $X_{2}$ will be > 0. But how about with $X_{1}$ in model?    \n* Example 2: $Y$ = number of tackles by a football player in a season; $W$ and $H$ are his weight and height. Fitted regression model is $\\hat Y = \\beta_{0} + .50W - .10H$. How do we interpret $\\hat\\beta_{2} < 0$?   \n\n###Two quotes by famous Statisticians   \n\"Essentially, all models are wrong, but some are useful.\"\n- George Box    \n\"The onle way to find out what will happen when a complex system is disturbed is to disturb the system, not merely to observe it passively\"   \n-Fred Mostellyer and John Tukey     \n\n###Estimation and Prediction for Multiple Regression   \n* Given estimates $\\hat\\beta_{0},\\hat\\beta_{1},...\\hat\\beta_{p}$, we can make predictions using the formula   \n$$ \\hat y = \\hat\\beta_{0} + \\hat\\beta_{1}x_{1} + \\hat\\beta_{2}x_{2}+...+\\hat\\beta_{p}x_{p} $$.    \n* We estimate $\\beta_{0},\\beta_{1},...,\\beta_{p}$ as the values that minimize the sum of squared residuals   \n$$ RSS = \\sum_{i=1}^n (y_{i} - \\hat y_{i})^2 $$   \n$$ = \\sum_{i=1}^n (y_{i} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i1} - \\hat\\beta_{2}x_{i2} - ... - \\hat\\beta_{p}x_{ip})^2 $$    \nThis is done using standard statistical software. The values $\\hat\\beta_{0},\\hat\\beta_{1},...,\\hat\\beta_{p}$ that minimizes RSS are the multiple least squares regression coefficient estimates.   \n\n\n##Some important Questions\n***\n1. *Is at least on of the predictors $X_{1},X_{2},...X_{p}$ usefule in predicting the response?*       \n2. *Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?*   \n3. *How well does the model fit the data?*     \n4. *Given a set of predictor values, what response value should we predict, and how accurate is our prediction?*    \n\n###Is at least one predictor useful?    \nFor the first question, we can use the F statistic    \n$$ F = \\frac{(TSS - RSS)/p}{RSS/(n - p - 1)} ~ F_{p,n-p-1} $$    \nexample:    \nQuantity | Value\n---------|----------\nResidual Standard Error | 1.69   \n$R^2$ | 0.897 \nF-Statistic | 570    \n\n###Deciding on the important variables   \n* The most direct approach is called *all subsets* or *best subsets* regression: we compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size.     \n* However we often can't examine all possible models, since they are $2^p$ of them; for example when $p = 40$ there are over a billion models!!!!     \n* Instead we need an automated approach that searches through a subset of them. We discuss two commonly used approaches next.    \n\n###Forward selection    \n* Begin with the *null model* -  a model that contains an intercept but no predictors.   \n* Fit $p$ simple linear regressions and add to the null model the variable that results in the lowest RSS. \n* Add to that model the variable that results in the lowest RSS amongst all two-variable models.    \n* Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold.    \n\n###Backward selection    \n* Start with all variables in the model.  \n* Remove the variable with the largest p-value -- that is, the variable that is the least statistically significant.   \n* The new $(p -1)$-variable model is fit, and the variable with the largest p-value is removed. \n* Continue until a stopping rule is reached. For instance, we may stop when all remaining variable have a significant p-value defined by some significance threshold.     \n\n###Model selection - continued   \n* Later we discuss more systematic criteria for choosing an \"optimal\" member in the path of models produced by forward or backward stepwise selection.   \n* These include *Mallow's $C_{p}$, Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted $R^2$* and *Cross-validation (CV)*    \n\n###Other Considerations in the Regression Model    \n*Qualitative Predictors*    \n* Some predictors are not *quantitative* but are *qualitative*, taking a discrete set of values.\n* These are also called *categorical* predictors or *factor variables.*    \n\n###Qualitative Predictors - contiued   \nExample: investigate differences in credit card balances between males and females, ignoring the other variables. We create a new variable    \n$$ x_{i} = \\begin{cases} \\beta_{0} + \\epsilon_{i}, &\\text{if i-th person is female} \\\\ \\beta_{0} + \\beta_{1} + \\epsilon_{i}, &\\text{if i-th person is male}  \\end{cases}  $$  \nResulting model:   \n$$ y_{i}=\\beta_{0} + \\beta_{1}x_{i} + \\epsilon = \\begin{cases} \\beta_{0} + \\epsilon_{i},  &\\text{if i-th person is female} \\\\ \\beta_{0} + \\beta_{1} + \\epsilon_{i}, &\\text{if i-th person is male} \\end{cases} $$   \nIntrepretation?    \n\n###Qualitative predictors with more than two levels   \n* With more than two levels, we create additional dummy variables. For example, for the *ethnicity* variable we create two dummy variables.  \n* Then both of these variables can be used in the regression equation, in order to obtain the model\n$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i1}+\\beta_{2}x_{i2} + \\epsilon = \\begin{cases} \\beta_{0} + \\beta_{1} + \\epsilon_{i}, &\\text{if i-th person is Asiab} \\\\ \\beta_{0} + \\beta_{2} + \\epsilon_{i}, &\\text{if i-th person is Caucasian} \\\\ \\beta_{0} + \\epsilon_{i}, &\\text{if i-th person is AA.} \\end{cases}  $$   \n* There will always be one fewer dummy variables than the number of levels. the level with no dummy variable - African American in this example - is known as the *baseline*.     \n\n##Extensions of the linear model\n***\nRemoving the additive assumption: *interactions* and *nonlinearity*     \n*Interactions:*    \n* In our previous analysis of the **Advertising** data, we assumed that the effect on **sales** of increasing one advertising medium is independent of the amount spent on the other media. \n* For example, the linear model \n$$ \\hat\\text{sales} = \\beta_{0} + \\beta_{1} \\times TV + \\beta_{2} \\times radio + \\beta_{3} \\times newspaper $$   \nstates that the everage effect on **sales** of a one-unit increase in **TV** is always \\beta_{1}, regardless of the amount spent on **radio**.    \n\n###Interactions - continued   \n* But suppose that spending money on radio adverstising actually increases the effectiveness of TV advertising, so that the slope term for **TV** should increase as **radio** increases.    \n* In this situation, given a fixed budget of $100,000, spending half on **radio** and half on **TV** may increase **sales** more than allocating the entire amount to either **TV** or to **radio**.  \n* In marketing, this is known as a *synergy* effect, and in statistics it is referred to as an *interaction* effect.    \n\n###Interpretation   \n* The results in this table suggest that interactions are important.   \n* The p-value for the interactio term $TV \\times radio$ is extremely low, indicating that there is strong evidence for $H_{A} : \\beta_{3} \\neq 0$.    \n* $R^2$ for the interaction model is 96.8%, compared to only 89.7% for the model that predicts **sales** using **TV** and **radio** without an interaction term.    \n\n###Interpretation - continued   \n* This means that (96.8 - 89.7) / (100 - 89.7) = 69% of the variability in **sales** that remains after fitting the additivie model has been explained by the interaction term.    \n* The coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increaed sales of   \n$(\\hat\\beta_{1} + \\hat\\beta_{3} \\times \\text{radio}) \\times 1000 = 19 + 1.1 \\times \\text{radio}$ units.    \n* An increase in radio advertising of $1,000 will be associated with an increase in sales of $(\\hat\\beta_{2} + \\hat\\beta_{3}) \\times 1000 = 29 + 1.1 \\times TV$ units.    \n\n###Hierarchy    \n* Somtimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, **TVT** and **radio**) so not.    \n* The *hierarchy principle*:   \n    * *If we include an interaction in a model, we should also include teh main effects, even if the p-values associated with their coefficients are not significant.*     \n    \n###Hierarchy - continued    \n* The rationale for this principle es that interactions are hard to interpret in a model without main effects - their meaning is changed.   \n* Specifically, the interaction terms also contain main effects if the model has no main effect terms.     \n\n###Interactions between qualitative and quantitative variables   \nConsider the **Credit** data set, and suppose that we wish to predict **balance** using **income** (quantitative) and **Student** (qualitative). \n$$  balance_{1} \\approx \\beta_{0} + \\beta_{1} \\times income_{i} + \\begin{cases} \\beta_{2}, &\\text{if i-th person is a student} \\\\ 0, &\\text{if i-th person is not a student} \\end{cases} $$  \n$$ = \\beta_{1} \\times income_{i} + \\begin{cases} \\beta_{0} + \\beta_{2}, &\\text{if i-th person is a student} \\\\ \\beta_{0}, &\\text{if i-th person is not a student} \\end{cases} $$   \n\nWith interactions, it take the form:   \n$$ balance_{1} \\approx \\beta_{0} + \\beta_{1} \\times income_{i} + \\begin{cases} \\beta_{2} + \\beta_{3} \\times income, &\\text{if student} \\\\ 0, &\\text{if  not a student} \\end{cases} $$  \n$$ =\\begin{cases} (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\times income_{i}, &\\text{if student} \\\\ \\beta_{0} + \\beta_{1} \\times income_{i}, &\\text{if i-th person is not a student} \\end{cases} $$    \n\n###Generalization of Linear Model    \nIn much of the rest of this course, we discuss methods that expand the scope of linear models and how they are fit:    \n* *Classification problems*: logistic regression, suppot vector machines    \n* *Non-linearity*: kernel smoothing, splines and generalized additive models, nearest neighbor methods.   \n* *Interactions*: Tree-based methods, bagging, random forests and boosting ( these capture non-linearities)   \n* *Regularized fitting*: Ridge regression and lasso     \n\n##Linear Regression with R\n```{r echo = FALSE}\nlibrary(MASS)\nlibrary(ISLR)\n###simple linear regression\nnames(Boston)\n?Boston\nplot(medv~lstat, Boston)\nfit1 = lm(medv~lstat, data=Boston)\nfit1\nsummary(fit1)\nabline(fit1,col=\"red\")\nnames(fit1)\nconfint(fit1) #confidence interval\npredict(fit1, data.frame(lstat=c(5,10,15)), interval=\"confidence\")\n### Multiple Linear Regression\nfit2=lm(medv~lstat+age, data=Boston)\nsummary(fit2)\nfit3=lm(medv~. , Boston) # use all other variables\nsummary(fit3)\nplot(fit3) \nfit4=update(fit3, ~.-age-indus)\nsummary(fit4)\n### Nonlinear terms and Interactions  \nfit5=lm(medv~lstat*age,Boston)\nsummary(fit5)\nfit6=lm(medv~lstat +I(lstat^2),Boston);summary(fit6)\nattach(Boston) #named variables are aviable in the environment\nplot(medv~lstat)\n#Can't use abline as it only works with straight line fit, instead we will use points using the fitted data from the model\npoints(lstat, fitted(fit6), col=\"red\", pch=20)\nfit7=lm(medv~poly(lstat,4))\npoints(lstat, fitted(fit7), col=\"blue\", pch=20)\n#Simply way of seeing all the aviable plotting characters   \nplot(1:20,1:20,pch=1:20,cex=2)\n###Qualitative predictor   \n#fix(Carseats) # open ups a graphical dataframe\nnames(Carseats)\nsummary(Carseats)\nfit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)\nsummary(fit1)\ncontrasts(Carseats$ShelveLoc) # How the variable is coded in a linear model\n###Writing R Functions  \nregplot=function(x,y){\n    fit=lm(y~x)\n    plot(x,y)\n    abline(fit,col=\"red\")\n}\nattach(Carseats)\nregplot(Price,Sales)\nregplot=function(x,y,...){\n    fit= lm(y~x)\n    plot(x,y,...)\n    abline(fit,col=\"red\")\n}\nregplot(Price, Sales, xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n```\n\n#Chapter 4 - Classification   \n*** \n##Introduction to Classification Problems\n***   \nClassification    \n* Qualitative variables take values in an ordered set $C$, such as: \n$$ \\text{eye Color} \\in {brown,blue,green} $$\n$$ \\text{email } \\in {spam, ham} $$    \n* Given a featrue vector $X$ and a qualitative response $Y$ taking values in the set $C$, the classification task is to build a function $C(X)$ that takes as input the feature vector $X$ and predicts its value for $Y$; i.e. $C(X) \\in C$.    \n* Often we are more interested in estimating the *probabilities* that $X$ belongs to each category $C$.    \n* For example, it's more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification fraudulent or not.   \n\n###Can we use Linear Regression?    \nSuppose for the **Default** classification task we code   \n$$ Y = \\begin{cases} 0, &\\text{if NO} \\\\ 1, &\\text{if YES} \\end{cases} $$   \nCan we simply perform a linear regressio of $Y$ on $X$ and classify as *Yes* if $\\hat Y > 0.5$?   \n\n* In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to *linear discriminant anaylysis* which we discuss later.    \n* Since in the population $E(Y\\mid X = x) = PR(Y = 1 \\mid X = x)$, we might think that regression is perfect for this task.    \n* However, *linear* regression might produce probabilities less than zero or bigger than one. *Logistic regression* is more appropriate.    \n\n\n###Linear Regressiom continued    \nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.    \n$$ Y = \\begin{cases} 1, &\\text{if stroke} \\\\ 2, &\\text{if drug overdose} \\\\ 3, &\\text{if epileptic seizure} \\end{cases}  $$   \nThis coding suggest on ordering, and in fact implies that the difference between **stroke** and **drug overdose** is the same as between **drug overdose** and **epileptic seizure**.      \n\n##Logistic Regression    \n*** \nLet's write $p(X) = Pr(Y = 1 \\mid X)$ for short and consider using **balance** to predict **default**. Logistic regression uses the form   \n$$ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}}  $$   \n($e \\approx 2.71828$ is a mathematical constant [Euler's number.]) It is easy to see that no matter what values $\\beta_{0},\\beta_{1}$ or $X$ take, $p(X)$ will have values between 0 and 1.     \n\nA bit of rearrangement gives \n$$log(\\frac{p(x)}{1 - p(x)}) = \\beta_{0} + \\beta_{1}X $$    \nThis monotone transformation is called the ***log odds*** or ***logit*** transformation of $p(x)$.     \n\n###Maximum Likelihood     \nWe use maximum likelihood to estimate the parameters.    \n$$ \\mathbb{l}(\\beta_{0},\\beta) = \\prod_{i:y_{i}=1}p(x_{i}) \\prod_{i:y_{i}=0} (1 - p(x_{i})) $$   \nThis *likelihood* gives the probability of the observed zeros and ones in the data. We pick $\\beta_{0}$ and $\\beta_{1}$ to maximize the likelihood of the observed data.    \n\nMost statistical packages can fit linear logistic regression models by maximum likelihood. In **R** we use the **glm** function.    \n\n###Making Predictions    \nWhat is our estimated probability of **default** for someone with a balance of $1000?     \n$$ \\hat p(X) = \\frac{e^{\\hat\\beta_{0}+\\hat\\beta_{1}X}}{1 + e^{\\hat\\beta_{0}+\\hat\\beta_{1}X}} = \\frac{e^{-10.6513+0.0055\\times1000}}{1 + e^{-10.6513+0.0055\\times1000}} = 0.006 $$    \nWith a balance of $2000?    \n$$ \\hat p(X) = \\frac{e^{\\hat\\beta_{0}+\\hat\\beta_{1}X}}{1 + e^{\\hat\\beta_{0}+\\hat\\beta_{1}X}} = \\frac{e^{-10.6513+0.0055\\times2000}}{1 + e^{-10.6513+0.0055\\times2000}} = 0.586 $$   \n\nLets do it again, using **Student** as the predictor.     \n$$\\widehat{PR}(default=Yes\\mid student=Yes) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}} = 0.0431$$,\n$$\\widehat{PR}(default=Yes\\mid student=Nes) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}} = 0.0431$$    \n\n##Multivariate Logistic Regression    \n***   \nLogistic regression with several variables   \n$$ log(\\frac{p(X)}{1-p(X)}) = \\beta_{0} + \\beta_{1}X_{1}+...+\\beta_{p}X_{p} $$    \n$$ p(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X_{1}+...+\\beta_{p}X_{p}}}{1 + e^{\\beta_{0}+\\beta_{1}X_{1}+...+\\beta_{p}X_{p}}} $$    \n\n###Confounding    \n* Students tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.   \n* But for each level of balance, students default less than non-students.   \n* Multiple logistic regression can tease this out.     \n\n##Logistic Regression-Case-Control Sampling and Multiclass   \n***   \n* In South African data, there are 160 cases, 302 controls $\\hat\\pi = 3.5$ are cases. Yet the prevelance of MI(Myocardio Infarction) in this region is $\\pi = 0.05$.    \n* With case-control samples, we can estimate the regression parameters $\\beta_{j}$ accurately ( if our model is correct); the constant term $\\beta_{0}$ is incorrect.   \n* We can correct the estimated intercept by a simple transformation    \n$$ \\hat\\beta_{0}^* = \\hat\\beta_{0} + log\\frac{\\pi}{1 - \\pi} - log\\frac{\\hat\\pi}{1 - \\hat\\pi} $$   \n* Often cases are rare and we take them all; up to five times that number of controls is sufficient.   \n\n###Diminishing returns in unbalanced binary data\n* Sampling more controls than cases reduces the variance of the parameter estimates. But after a ration of about 5 to 1 the variance reduction flattens out.    \n\n###Logistic regression with more than two classes   \nSo far we have discussed logistic regression with two classes. It is easily generalized to more than two classes. One version (used in the R package **glmnet**) has the symmetric form     \n$$ Pr(Y = k \\mid X) = \\frac{e^{\\beta_{0k}+\\beta_{1k}X_{1}+...+\\beta_{pk}X_{p}}}{\\sum_{l=1}^K e^{\\beta_{0l}+\\beta_{1l}X_{1l}+...+\\beta_{pl}X_{p}} $$    \nHere there is a linear function for *each* class.    \n\nMulticlass logistic regression is also referred to as *multinomial regression*.    \n\n##Discrimant Analysis\n***     \nHere the approach is to model the distribution of $X$ in each of the classes separately, and then use *Bayes theorem* to flip things around and obtain $Pr(Y\\mid X)$.      \nWhen we use normal (Gaussain) distributions for eachg class, this leads to linear or quadratic discriminant analysis.     \nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions.      \n\n###Bayes theorem for classification    \nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probablistic modeling. Here we focus on a simple result, known as Bayes theorem:     \n$$Pr(Y = k \\mid X = x) = \\frac{Pr(X = x \\mid Y = k) \\cdot Pr(Y = l)}{Pr(X = x)} $$    \nOne writes this slightly differently for discriminant analysis:    \n$$ Pr(Y = k \\mid X = x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^K \\pi_{l}f_{l}(x)} $$, where \n* $f_{k}(x) = Pr(X = x \\mid Y = k)$ is the *density* for $X$ in class $k$. Here we will use normal densities for these, seperately in each class.    \n* $\\pi_{k} = Pr(Y = k)$ is the marginal or *prior* probability for class *k*.      \n\n###Classify to the highest density   \nWe classify a new point according to which density is highest.    \nWhen the priors are different, we take them into account as well, and compare $\\pi_{k}f_{k}(x)$. On the right, we favor the pink class -  the decision boundary has shifted to the left.    \n\n###Why discriminant analysis?    \n* When the classes are well-seperated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.    \n* If $n$ is small and the distribution of the predictors $X$ is approximately normal is each of the classes, the linear discriminant model is again more stable than the logistic regression model.   \n* Linear disriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.    \n\n##Gaussian Discriminant Anaylsis - One variable   \n***   \n###linear Discriminant Analysis when p =1    \nThe Gausssian density has the form    \n$$ f_{k}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}}}e^{-\\frac{1}{2}(\\frac{x - \\mu_{k}}{\\sigma_{k}})^2} $$   \nHere $\\mu_{k}$ is the mean, and $\\sigma_{k}^2$ the variance (in class $k$). We will assume that all the $\\sigma_{k} = \\sigma$ are the same.    \n\nPlugging this into Bayes formula, we get a rather complex expression for $p_{k}(x) = Pr(Y = k \\mid X = x)$:    \n$$ p_{k}(x)=\\frac{\\pi_{k}\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}(\\frac{x-\\mu_{k}}{\\sigma})^2}}{\\sum_{l=1}^K \\pi_{l}\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}(\\frac{x - \\mu_{l}}{\\sigma})^2}} $$    \nHappily, there are simplifications and cancellations.    \n\n###Discriminant functions    \nTo classify at the value of $X = x$, we need to see which of the $p_{x}(x)$ largest. Taking logs, and discarding terms that do not dpened on $k$, we see that this is equivalent to assigning $x$ to the class with the largest *discriminant score*:    \n$$ \\delta_{k}(x) = x \\cdot \\frac{\\mu_{k}}{\\sigma^2} - \\frac{\\mu_{k}^2}{2\\sigma^2} + log(\\pi_{k}) $$    \nNote that $\\delta_{k}(x)$ is a *linear* function of x.    \nIf there are $K =2$ classes and $\\pi_{1} = \\pi_{2} = 0.5$, then one can see that the *decision boundary* is at     \n$$ x = \\frac{\\mu_{1} + \\mu_{2}}{2} $$   \n\n###Estimating the parameters   \n$$ \\hat\\pi_{k} = \\frac{n_{k}}{n} $$   \n$$ \\hat\\mu_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k} x_{i} $$   \n$$ \\hat\\sigma^2 = \\frac{1}{n - K}\\sum_{k=1}^K\\sum_{i:y_{i}=k}(x_{i}-\\hat\\mu_{k})^2 $$    \n$$ = \\sum_{k=1}^K\\frac{n_{k} -1}{n - K} \\cdot \\hat\\sigma_{k}^2 $$   \nwhere $\\sigma_{k}^2 = \\frac{1}{n_{k}-1}\\sum_{i:y_{i}=k}(x_{i}-\\hat\\mu_{k})^2$ is the usual formula for the estimated variance in the *k*th class.    \n\n##Gaussian Discriminant Analysis - Many Variables    \n***    \n###Linear Discriminant Analysis when p > 1    \nDensity: $f(x) = \\frac{1}{(2\\pi)^{\\frac{p}{2}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{1}(x - \\mu)^T\\Sigma^-1(x - \\mu)}$    \nDiscriminant function: $\\delta_{k}(x) = x^T\\Sigma^{-1}\\mu_{k}-\\frac{1}{2}\\mu_{k}^T\\Sigma^{-1}\\mu_{k}+log(\\pi_{k})$    \nIt looks complex, but important thing to note is that it's, again, linear in x. Here's x alone multiplied by a coefficient vector($\\Sigma^{-1}\\mu_{k}$), and these ($-\\frac{1}{2}\\mu_{k}^T\\Sigma^{-1}\\mu_{k}+log(\\pi_{k})$) are all just constants. So this, again, is a linear function.   \n\nDespite its complex form,   \n$\\delta_{k}(x)= c_{k0}+c_{k1}x_{2}+...+c_{kp}x_{p}$ - a linear function.   \n\n###Fisher's Discriminant Plot   \nWhen there are $K$ classes, linear discriminant analysis can be viewed exactly in a $K-1$ dimensional plot. Why? Because it essentially classifies to the closest centriod, and they span a $K-1$ dimensional plane. \nEven when $K > 3$, we can fine the 'best' 2-dimensional plane for visualizing the discriminant rule.   \n\n###From $\\delta_{k}(x)$ to probabilities   \nOnce we have estimates $\\hat\\delta_{k}(x)$, we can turn these into estimates for class probabilities:    \n$$ \\widehat{Pr}(Y = k\\mid X = x) = \\frac{e^{\\hat\\delta_{k}(x)}}{\\sum_{l=1}^K e^{\\hat\\delta_{l}(x)}} $$   \nSo classifying the largest $\\hat\\delta_{k}(x)$ amounts to classifying to the class for which $ \\widehat{Pr}(Y = k \\mid X = x)$ is largest.    \nWhen $K = 2$, we classify to class 2 if $\\widehat{Pr}(Y = 2 \\mid X = x) \\geq 0.5$ else to class 1.    \n\n###LDA on Credit Data\n\n- | - | True | Default | Status\n--|---|------|---------|----------\n- | - | No | Yes |  Total \nPredicted | No | 9644 | 252 | 9896\nDefault | Yes | 23 | 81 | 104 \nStatus | Total | 9667 | 333 | 10000   \n\n(23 + 252)10000 errors -- a 2.75%  misclassification rate!   \nSome Caveats:  \n* This is *training* error, and we may be overfitting. Not a big concern here since n = 10000, and p = 4!   \n* If we classsified to the prior -- always to class **No** in this case --  we would make 333/10000 errors, or only 3.33%. This called the null rate or the naive classifier. This is the rate of the prior our model has to at least outperform this rate or we are just wasting our time.    \n* Of teh true **No**'s, we make 23/9667 = 0.2$ errors: of the true **Yes**'s, we make 252/333 = 75.7% errors!    \n\n###Types of errors    \n**False positive rate**: The fraction of negative examples that are classified as positive -- 0.2$ in example.   \n**False negative rate**: The fraction of positive examples that are classified as negative -- 75.7% in example.  \nWe produced this table by classifying to class **Yes** if    \n$$ \\widehat{Pr}(Default=Yes\\mid Balance,Student) \\geq 0.5$$   \nWe can change the two error rates by changing the threshold from 0.5 to some other value in [0,1]:    \n$$ \\widehat{Pr}(Default = Yes \\mid Balance, Student) \\geq threshold $$   \nand vary *threshold*.    \n\n###ROC Curve   \nThe ***ROC plot** displays both simultaneously    \nSometime we use the ***AUC*** or ***area under the curve** to summarize the overall performance. Higher ***AUC*** is good.   \n\n##Quadratic Discriminant Analysis and Niave Bayes\n***    \n$$ Pr(Y = k \\mid X = x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^K \\pi_{l}f_{l}(x)} $$ \n\nWhen $f_{k}(x)$ are Gaussian densities, with the same covariance matrix $\\Sigma$, this leads to linear discriminant analysis. By altering the forms for $f_{k}(x)$, we get different classifiers..  \n* With Gaussians but different $\\delta_{k}$ in each class, we get ***quadratic discriminant analysis***.   \n* With $f_{k}(x) = \\prod_{j=1}^p f_{jk}(x_{j})$ (conditional independence model) in each class we get *naive Bayes*. For Gaussian this means the $\\Sigma_{k}$ are diagonal.    \n* Many other forms, by proposing specific density models for $f_{k}(x)$, including nonparametric approaches.    \n\n###Quadratic Discriminant Analysis   \n$$ \\delta_{k}(x) = -\\frac{1}{2}(x - \\mu_{k})^T \\Sigma_{k}^{-1} (x - \\mu_{k}) + log(\\pi_{k}) - \\frac{1}{2}log|\\Sigma_{k}| $$  \nBecause the $\\Sigma_{k}$ are different, the quadratic terms matter.   \n\n###Naive Bayes   \nAssumes features are independent in each class.\nUseful when *p* is large, and so multivariate methods like QDA and even LDA break down.    \n* Gaussian naive Bayes assumes each $\\Sigma_{k}$ is diagonal:   \n$$ delta_{k}(x) \\propto log\\bigg[ \\pi_{k}\\prod_{j=1}^p f_{kj}(x_{j})\\bigg] $$ \n$$ = -\\frac{1}{2}\\sum_{j=1}^p\\Big[\\frac{(x_{j}-\\mu_{kj})^2}{\\sigma_{kj}^2} + \\simga_{kj}^2\\Big] + log\\pi_{k} $$   \n* can use for *mixed* feature vectors (qualitative and quantitative). if $X_{j}$ i qualitative, replace $f_{kj}(x_{j})$ with the probability mass function ( histogram) over discrete categories.   \nDespite strong assumptions, naive Bayes ofter produces good classification results.   \n\n###Logistic Regression versus LDA   \nFor a two-class problem, one can show that for LDA   \n$$ log\\big( \\frac{p_{1}(x)}{1 - p_{1}(x)}\\big) = log\\big(\\frac{p_{1}(x)}{p_{2}(x)}\\big) = c_{0} + c_{1}x_{1} +...+ c_{p}x_{p} $$   \nSo it has the same form as logistic regression.   \nThe difference is in how the parameters are estimated.    \n* Logistic regression uses the conditional likelihood based on $Pr(Y\\mid X)$ (known as *discriminant learning*).   \n* LDA uses the full likelihood based on $Pr(X,Y)$ (known as *generative learning*).     \n* Despite these differences, in practice the results are ofter very similar.    \nFootnote: logistic regression can also fit quadratic boundaries like QDA, by explicitly including quadratic terms in the model.    \n\n###Summary \n* Logistic regression is very popular for classification, especially when K=2.  \n* LDA is useful when $n$ is small, or the classes are well seperated, and Gaussian assumptions are reasonable. Also when $K > 2$.   \n* Naive Bayes is useful when $p$ is very large.   \n\n##Classification in R\n***\n```{r echo=FALSE}\nrequire(ISLR)\nnames(Smarket)\nsummary(Smarket)\n?Smarket\npairs(Smarket, col=Smarket$Direction)\n\n# Logistic regression\nglm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)\nsummary(glm.fit)\nglm.probs=predict(glm.fit, type=\"response\")\nglm.probs[1:5]\nglm.pred=ifelse(glm.probs > 0.5, \"Up\", \"Down\")\nattach(Smarket)\ntable(glm.pred, Direction)\nmean(glm.pred==Direction)\n\n# Make training and test set\ntrain = Year < 2005\nglm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = Smarket, family = binomial, subset = train)\nglm.probs = predict(glm.fit, newdata=Smarket[!train,], type=\"response\")\nglm.pred = ifelse(glm.probs > 0.5, \"Up\", \"Down\")\nDirection.2005 = Smarket$Direction[!train]\ntable(glm.pred, Direction.2005)\nmean(glm.pred==Direction.2005)\n\n# Fit smaller model\nglm.fit = glm(Direction~Lag1+Lag2, data = Smarket, family = binomial, subset = train)\nglm.probs = predict(glm.fit, newdata=Smarket[!train,], type=\"response\")\nglm.pred = ifelse(glm.probs > 0.5, \"Up\", \"Down\")\ntable(glm.pred, Direction.2005)\nmean(glm.pred == Direction.2005)\n\n```\n\n```{r echo=FALSE}\nrequire(ISLR)\nrequire(MASS)\n\n## Linear Discriminant Analysis  \nlda.fit = lda(Direction~Lag1+Lag2, data = Smarket, subset = Year < 2005)\nlda.fit\nplot(lda.fit)\nSmarket.2005 = subset(Smarket, Year==2005)\nlda.pred = predict(lda.fit, Smarket.2005)\n#lda.pred[1:5,]\nclass(lda.pred)\ndata.frame(lda.pred)[1:5,]\ntable(lda.pred$class, Smarket.2005$Direction)\nmean(lda.pred$class == Smarket.2005$Direction) # b/c true and false are coerced to 1 and 0.   \n\n```\n\n```{r echo = FALSE}\n## K-Nearest Neighbors\nlibrary(class)\n?knn\nattach(Smarket)\nXLag=cbind(Lag1,Lag2)\nXLag[1:5,]\ntrain = Year < 2005\nknn.pred = knn(XLag[train,], XLag[!train,], Direction[train], k = 1)\ntable(knn.pred, Direction[!train])\nmean(knn.pred == Direction[!train])\n\n## k = 3\nknn.pred = knn(XLag[train,], XLag[!train,], Direction[train], k = 3)\ntable(knn.pred, Direction[!train])\nmean(knn.pred == Direction[!train])\n\n```\n\n#Chapter 5 Resampling Methods \n***\n\n##Cross-Validation \n***\n\n###Cross-validation and the Bootstrap   \n* In the section we discuss two *resampling* methods: cross-validation and the bootstrap.    \n* These methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model. \n* For example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates   \n\n###Training Error versus Test error     \n* Recall the distinnction between the *test error* and the *training error*:      \n* The *test error* is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.    \n* In contrast, the *training error* can be easily calculated by applying the statistical learning method to the observations used in its training.   \n* But the training error rate often is quite different from the test eoor rate, and in particular the former can *dramatically underestimate* the latter.      \n\n###More on prediction-error estimates    \n* Best solution: a large designated test set. Often not avialable. \n* Some methods make a *mathematical adjustment* to the training error rate in order to estimate and test error rate. These include the $C_{p}$ statistic, *AIC* and *BIC*. They are discussed elsewhere is this course.    \n* Here we instead consider a class of methods that estimate the test error by *holding out* a subset of the training observations from the fitting process, and teh applying the statistical learning method to those held out observations.     \n\n###Validation-set approach    \n* Here we randomly divide the avialable set of sample into two parts: a *training set* and a *validation* or *hold-out set*.    \n* The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.     \n* The resulting validation-set error provides an estimate of the test error. This is typically assessed used MSE in the case of a quantitative response and misclassification rate in the case of a qualitative (discrete) response.     \n\n###Drawbacks of validation set approach    \n* The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.     \n* In the validation approach, only a subset of the observations -- those that are included in the training set rather than in the validation set -- are used to fit the model.     \n* This suggests that the validation set error may tend to *overestimate* the test error for the model fit on the entire data set.      \n\n##K-fold Cross-Validation    \n***   \n* *Widely used approach* for estimating test error.     \n* Estimates can be used to select the best model, and to give an idea of the test error of the fina chosen model.    \n* Idea is to randomly divide the data into K equal-sized parts. We leave out part K, fit the model to the other K -1 parts (combined), and then obtain predictions for the left-out kth part.    \n* This is done in turn for each part $k=1,2,...K,$ and then the results are combined.     \n\n###The details   \n* Let the $K$ parts be $C_{1},C_{2},...C_{K}$ where $C_{k}$ denotes the indices of the observations in part $k$. Ther are $n_{k}$ observations in part $k$: if $N$ is a multiple of $K$, then $n_{k}=n/K$.     \n* Compute   \n$$ CV_{(K)}=\\sum_{k=1}^K \\frac{n_{k}}{n}MSE_{k}  $$   \nwhere $MSE_{k}=\\sum_{i\\in C_{k}}(y_{i}-\\hat y_{i})^2 /n_{k}$, and $\\hat y_{i}$ is the fit for observation $i$, obtained from the data with part $k$ removed.    \n* Setting $K=n$ yields n-fold or *leave-one out cross-validation* (LOOCV).  \n\n###A nice special case!    \n\n* With least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:   \n$$ CV_{(n)}=\\frac{1}{n}\\sum_{i=1}^n(\\frac{y_{i}-\\hat y_{i}}{1 - h_{i}})^2, $$   \nwhere $\\hat y_{i}$ is the ith fitted value from the original least squares fit, and $h_{i}$ i the leverage (diagonal of the \"hat\" matrix). This is like the ordinary MSE, except the ith residual is divided by $1-h_{i}$.     \n* LOOCV sometimes useful, but typically doesn't *shake up* the data enough. THe estimates from each fold are highly correlated and hence their average can have high variance. \n* A better choice is $K = 5$ or 10.     \n\n###Other issues with Cross-validation    \n* Since each training set is only  $(K-1)/K$ as big as the original training set, the estimates of prediction error will typically be biased upward.     \n* This bias is minimized when $K = n$ (LOOCV), but this estimate has high variance, as noted earlier.   \n* $K = 5$ or 10 provides a good compromise for this bias-variance tradeoff.    \n\n###Cross-Validation for Classification Problems    \n* We divide the data into K roughly equal-sized parts $C_{1},C_{2},...C_{K}$. $C_{k}$ denotes the indices of the observations in part $k$. There are $n_{k}$ observations in part $k$: if $n$ is a multiple of $K$, then $n_{k} = \\frac{n}{K}$.   \n* Compute   \n$$ CV_{K} = \\sum_{k=1}^K \\frac{n_{k}}{n} Err_{k} $$    \nwhere $Err_{k} = \\sum_{i\\in C_{k}} I(y_{i} \\neq \\hat y_{i}) / n_{k}$. \n* The esitmated standard deviation of $CV_{k}$ is   \n$$ \\widehat{SE}(CV_{K}) = \\sqrt{\\sum_{k=1}^K (Err - \\bar Err_{k})^2 / (K -1)} $$    \n* This is a useful estimate, but strictly speaking, not quite valid.   \n\n##Cross-Validation: the wrong and right way   \n***   \n* Consider a simple classifier applied to some two-class data:    \n    1.  Starting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with  the class labels.   \n    2. We then apply a classifier such as logistic regression, using only these 100 predictors.    \nHow do we estimate the test set performance of this classifier?   \n\nCan we apply cross-validation in step 2, forgetting about step1?    \n\n***NO!***      \n* This would ignore the fact that in Step 1, the procedure *has already seen the labels of the training data*, and made use of them. This is a form of training and must be inclued in the validation process.    \n* It is easy to simulate realistic data with the class labels independent of the outcome, so that true test error = 50% but the CV error estimate that ignores Step 1 is zero!     \n\n###The Wrong and Right Way    \n* ***Wrong***: Apply cross-validation is step 2.  \n* ***Right***: Apply cross-validation to steps 1 and 2.   \n\n##The Bootstrap   \n***    \n* The *bootstrap* is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.   \n* For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.     \n\n###Where does the name come from?     \n* The use of the term bootstrap derives from the phrase *to pull oneself up by one's bootstraps*, widely tought to be based on one of the eighteenth century \"The Suprising Adventurs of Baron Munchausen\" by Rudolph Erich Raspe: *\"The Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by is own bootstraps.\"*     \n* It is not the same as the term \"bootstrap\" used in computer science meaning to \"boot\" a computer from a set of core instructions, though the derivation is similar.      \n\n###A simple example    \n* Suppose that we wish to invest a fixed sum of money in two financial assests that yield returns of X and Y, respectively, where X and Y are random quantities.    \n*  We will invest a fraction $\\alpha$ of our money in X, and will invest the remaining $1-\\alpha$ in Y.    \n* We wish to choose $\\alpha$ to minimize the total risk, or variance, of our investment. In other words, we want to minimize $Var(\\alpha X + (1 - \\alpha) Y)$.      \n* One can show that the value that minimizes the risk is given by:   \n$$ \\alpha = \\frac{\\sigma_{Y}^2 - \\simga_{XY}}{\\simga_{X}^2 + \\simga_{Y}^2 - 2\\sigma_{XY}} $$,    \nwhere $\\simga_{X}^2 = Var(X)$, $\\simga_{Y}^2 = Var(Y)$, and $\\simga_{XY} = Cov(X,Y)$.       \n* But the values of $\\simga_{X}2,\\simga_{Y}^2$, and $\\simga_{XY}$ are unknown.    \n* We can compute estimates for these quantities,$\\simga_{X}2,\\simga_{Y}^2$, and $\\simga_{XY}$, using a data set that contains measurements for X and Y. \n* We can then estimate the value of $\\alpha$ that minimizes the variace of our invest using:     \n$$ \\hat\\alpha = \\frac{\\hat\\sigma_{Y}^2 - \\hat\\simga_{XY}}{\\hat\\simga_{X}^2 + \\hat\\simga_{Y}^2 - 2\\hat\\sigma_{XY}} $$      \n* To estimate the standard deviation of $\\hat\\alpha$, we repeated the process of simulating 100 required paired observations of X and Y, and estimating $\\alpha$ 1,000 times.   \n* We thereby obtain 1,000 estimates for $\\alpha$ which we can call $\\hat\\alpha_{1},\\hat\\alpha_{2},...,\\hat\\alpha_{1000}$.     \n* The left-hand panel of the Figure on slide 29 displays a histogram of the resulting estimates.    \n* For these simulations the parameters were set to $\\sigma_{X}^2 = 1$, $\\simga_{Y}^2$, and $\\sigma_{XY} = 0.5$, and so we know that the true value of $\\alpha$ is 0.6 (indicated by the red line).     \n* The mean over all 1,000 estimates for $\\alpha$ is     \n$$ \\bar\\alpha=\\frac{1}{1000}\\sum_{r=1}^1000 \\hat\\alpha_{r} = 0.5996 $$,   \nvery close to $\\alpha=0.6$, and the standard deviation of the estimate is    \n$$ \\sqrt{\\frac{1}{1000-1}\\sum_{r=1}^1000 (\\hat\\alpha_{r} - \\bar\\alpha)^2} =0.083 $$     \n* This gives us a very good idea of the accuracy of $\\hat\\alpha$: $SE(\\hat\\alpha) \\approx 0.083$.    \n* So roughly speaking , for a random sample from the population, we would expect $\\hat\\alpha$ to differ from $\\alpha$ by approximately 0.008, on average.     \n\n###Now back to the real world    \n* The procedure outline above cannot be applied, because for real data we cannot generate new samples from the original population.    \n* However, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.   \n* Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set *with replacement*.    \n* Each of these \"bootstrap data sets\" is created by sampling *with replacment*, and is the *same size* as our original dataset. As a result some observations may appear more than once is a given bootstrap data set and some not at all.     \n\n##More on the Bootstrap    \n***   \n###The bootstrap in general    \n* In more complex data situations, figuraing out the appropriate way to generate boostrap sampls can require some thought.    \n* For example, if the data is a time series, we can't simply sample the observations with replacement (*why not?*).   \n    * Solution: the Block bootstrap, dived the data up into blocks and assumes that things are independent betweeen blocks.\n* We can instead create blocks of consecutive observations, and sample those with replacements. Then we paste together sample blocks to obtain a bootstrap dataset.  \n\n###Other uses of the bootstrap    \n* Primarily used to obtain standard errors of an estimate.   \n* Also provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the Figure on slide 29, the 5% and 95% quantiles of the 1000 values is (.43,.72).   \n* This represents an approximate 90% confidence interval for the true $\\alpha$. * How do we interpret this confidence interval?*    \n* The above interval is called a *Boostrap Percentile* confidence interval. It is the simplest method ( among many approaches) for obtaining a confidence interval from the bootstrap.    \n\n###Can the bootstrap estimate prediction error?   \n* In cross-validation, each of the K validation folds is distinct from the other K-1 folds used for training: *There is no overlap*. This is crucial for its success.   \n* To estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our trainin sample, and the original sample as our validation sample.    \n* But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. \n* This will cause the bootstrap to seriously underestimate the true prediction error.    \n* The other way around -- with original sample - training sample, bootstrap dataset+ validation sample - is worse!    \n\n###Removing the overlap     \n* Can partly fix this problem by only using predictions for those observation that did not (by chance) occur in the current bootstrap sample.   \n* But the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error.   \n\n##Resampling in R\n*** \n\n```{r echo = FALSE}\nrequire(ISLR)\nrequire(boot)\n?cv.glm\nnames(Auto)\nplot(mpg~hp,data=Auto)\n## LOOCV \nglm.fit = glm(mpg~hp, data = Auto)\ncv.glm(Auto,glm.fit)$delta\n\n##Lets write a simple function to use formula (5.2)\nloocv = function(fit){\n    h = lm.influence(fit)$h\n    mean((residuals(fit)/(1-h))^2)\n}\n\n## Now we try it out\n\nloocv(glm.fit)\n\n\ncv.error = rep(0,5)\ndegree=1:5\nfor(d in degree){\n    glm.fit=glm(mpg~poly(hp,d), data=Auto)\n    cv.error[d]=loocv(glm.fit)\n}\nplot(degree, cv.error, type=\"b\")\n\n## 10-fold CV   \n\ncv.error10 = rep(0,5)\nfor(d in degree){\n    glm.fit = glm(mpg~poly(hp,d), data=Auto)\n    cv.error10[d] = cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\nlines(degree, cv.error10, type = \"b\", col = \"red\")\n\n##Bootstrap\n## Minimum risk investment\n\nalpha = function(x,y){\n    vx = var(x)\n    vy = var(y)\n    cxy = cov(x,y)\n    (vy - cxy)/(vx+vy-2*cxy)\n}   \nalpha(Portfolio$X,Portfolio$Y)\n\n## What is the standard error of alpha?\n\nalpha.fn=function(data, index){\n    with(data[index,],alpha(X,Y))\n}\n\nalpha.fn(Portfolio,1:100)\n\nset.seed(1)\nalpha.fn(Portfolio,sample(1:100,100,replace = TRUE))\n\nboot.out = boot(Portfolio,alpha.fn,R=1000)\nboot.out\nplot(boot.out)\n```\n\n##Chapter 5 Quiz\n*** \n```{r echo = FALSE}\n## To within 10%, what is the standard error for β1?\nload(\"5.R.RData\")\nnames(Xy)\nlm.fit = lm(y~X1+X2, data = Xy)\nsummary(lm.fit)\nmatplot(Xy,type=\"l\")\n\n## Now, use the (standard) bootstrap to estimate s.e.(β̂ 1). To within 10%, what do you get?\nalpha.fn = function(data, index){\n    data = data[index,]\n    lm(y~X1+X2, data = data)$coefficients[2]\n    \n}\nsummary(lm(y~X1+X2,data = Xy))\n\nboot.out = boot(Xy, alpha.fn, R=2000)\nboot.out\n\n## Finally, use the block bootstrap to estimate s.e.(β̂ 1). Use blocks of 100 contiguous observations, and resample ten whole blocks with replacement then paste them together to construct each bootstrap time series.\n\n# Block into 100 contiguous rows by using a list of vectors\nnew.Xy = list(c(0:100), c(101:200), c(201:300), c(301:400), c(401:500), c(501:600), c(601:700), c(701:800), c(801:900), c(901:1000))\n\n# random draw with bootstrap the vectors in the list and then unlist them into one vector for the lm() function.\nalpha.fn = function(data, index){\n    data = data[index]\n    d = Xy[unlist(data),]\n    lm(y~X1+X2, data = d)$coefficients[2] \n}\n#unlist(new.Xy[c(1,2,3)])\n#as.vector(rbind(a,b))\nboot.out = boot(new.Xy, alpha.fn, R=2000)\nboot.out\n```\n\n#Chapter 6 Linear Model Selection and Regularization     \n***     \n##Introduction and Best-Subset Selection   \n***   \n###Linear Model Selection and Regularization    \n* Recall the linear model    \n$$ Y = \\beta_{0} + \\beta_{1}X_{1}+...+\\beta_{p}X_{p} + \\epsilon $$    \n* In the lectures that follow, we consider some approaches extending the linear model framework. In the lectures covering Chapter 7 of the text, we generalize the linear model in order to accomodate *non-linear*, but still *additive*, relationships.     \n* In the lectures covering Chapter 8 we consider even more general *non-linear*  models.     \n\n###In praise of linear models!    \n* Despite its simplicity, the linear model has distinct advantages in terms of its *iterpretability* and ofter shows good *predictive performance.*     \n* Hence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures.     \n\n###Why consider alternatives to least squares?     \n* *Prediction Accuracy*: especially when $p > n$, to control the variance.    \n* *Model Interpretability:* By removing irrelevant features - that is, by setting the corresponding coefficient estimates to zero -  we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing *feature selection*.      \n\n###Three classes of methods    \n* *Subset Selection*: We identify a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.     \n* *Shrinkage*. We fit a model involving all $p$ predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as *regularization*) has the effect of reducing variance and can alsos perfom variable selection.    \n* *Dimension Reduction*. We project the $p$ predictors into a M-dimensional subspace, where $M < p$. This is achieved by computing M different *linear combinations*, or *projections*, of the variables. Then these M projections are used  as predictors to fit a linear regression model by least squares.    \n\n###Subset Selection    \n*Best subset and stepwise model selection procedures*     \n\n**Best Subset Selection**   \n1. Let $M_{0}$ denote the *null model*, which contains no predictors. This model simply predicts the sample mean for each observation.    \n2. for $k = 1,2,..p$:    \n    a. Fit all $ p \\choose k$ models that contain exactly $k$ predictors. \n    b. Pick the best among these $ p \\choose k$ models, and call it $M_{k}$. Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.    \n3. Select a single best model among $M_{0},...,M_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$.     \n\n##Stepwise Selection\n***    \n###Extensions to other models    \n*Although we have presented best subset selection here for least squares regression, the same ideas apply to other types of models, such as logistic regression.   \n* The *devience*-negative two times the maximized log-likelihood - plays the role of RSS for a broader class of models.    \n\n###Stepwise Selection    \n* For computational reasons, best subset selection cannot be applied with very large $p$. *Why not?*    \n* Best subset selection may also suffer from statistical problems when *p* is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.   \n* Thus an enormous searcy space can lead to *overfitting* and high variance of the coefficient estimates.   \n* For both of these reasons, *stepwise* methods, which explore a far more restricted set of models, are attractive alternatives to best subst selection.     \n\n###Forward Stepwise Selection   \n* Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictiors are in the model.   \n* In particular, at each step the variable that gives the greatest *additional* improvement to the fit is added to the model.   \n\n###In Detail   \n\n**Forward Stepwise Selection**   \n1. Let $M_{0}$ denote the *null* model, which contains no predictors.   \n2. For $k = 0,...,p-1$:   \n    2.1 Consider all $p-k$ models that augment the predictors in $M_{k}$ with one additional predictor.   \n    2.2 Choose the *best* among these $p-k$ models, and call it $M_{k+1}$. Here *best* is defined as having smallest RSS or highest $R^2$.   \n3. Select a single best model from among $M_{0},...,M_{p}$ using cross-validated prediction error, $C_{P}$ (AIC), BIC, or adjusted $R^2$.     \n\n###More on Forward Stepwise Selection   \n* Computational advantage over best subset selection is clear.   \n* It is not guaranteed to find the best possible model out of all $2^P$ models containing subsets of the $p$ predictors. *Why not? Give an example*.     \n\n##6.3 Backward Stepwise Selection   \n***   \n* Like forward stepwise selection, *backward stepwise selection* provides an efficient alternative to best subset selection.    \n* However, unlike forward stepwise selection, it begins with the full least squares model containing all $p$ predictors, and then iteratively removes the least useful predictor, one-at-a-time.     \n\n###Backward Stepwise Selection: details   \n\n**Backward Stepwise Selection**   \n1. Let $M_{p}$ denote the *full* model, which contains all $p$ predictors.   \n2. For $k=p,p-1,...,1$:    \n    2.1 Consider all $k$ models that contain all but one of the predictors in $M_{k}$, for a total of $k-1$ predictors.   \n    2.2 Choose the *best* among these *k* models, and call it $M_{k-1}$. Here *best* is defined as having smallest RSS or highest $R^2$.    \n3. Select a single best model from among $M_{0},...M_{p}$ using cross-validated prediction error, $C_{P}$ (AIC), BIC, or adjusted $R^2$.     \n\n###More on Backward Stepwise Selection    \n* Like forward stepwise selection, the backward selection approach searches through only $1 + p(p+1)/2$ models, and so can be applied in settings where $p$ is too large to apply best subset selection     \n* Like forward stepwise selection, backward stepwise selection is not guaranteed to yield the *best* model containing a subset of the $p$ predictors.    \n* Backward selection requires that the *number of samples n is larger than the number of variables p* (so that the full model can be fit). In contrast, forward stepwise can be used even when $n>p$, and so is the only viable subset method when $p$ is very large.       \n\n###Choosing the Optimal Model     \n* The model containing all of the predictors will always have the smallest RSS and the largest $R^2$, since these quantities are related to the training error.     \n* We wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.    \n* Therefore, RSS and $R^2$ are not suitable for selecting the best model among a collection of models with different numbers of predictors.     \n\n##Estimating Test Error    \n***    \n###Estimating test error: two approaches    \n* We can indirectly estimate test error by making an *adjustment* to the training error to account for the bias due to overfitting.    \n* We can *directly* estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures.    \n* We illustrate both approaches next.     \n\n###$C_{p}$, AIC, BIC, and Adjusted $R^2$     \n* These techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.    \n* The next figure displays $C_{p}$, BIC, and adjusted $R^2$ for the best model of each size produced by best subset selection on the Credit data set.     \n\n###Now for some details    \n* *Mallows $C_{P}$:    \n$$ C_{P}=\\frac{1}{n}(RSS + 2d\\hat\\sigma_2) $$    \nwhere d is the total # of paramters used and $\\hat\\sigma^2$ is an estimate of the variance of the error $\\epsilon$ associated with each response measurement.    \n* The *AIC* criterion is defined for a large class of models fit by maximum likelihood:    \n$$ AIC=-2logL+2\\cdot d $$    \nwhere $L$ is the maximized value of the likelihood function for the estimated model.    \n* In the case of the linear model with Gaussain errors, maximum likelihood and least squares are the same thing, and $C_{p}$ and AIC are equivalent.\n\n###Details on BIC   \n$$ BIC = \\frac{1}{n}(RSS + log(n)d\\hat\\sigma^2) $$.    \n* Like $C_{p}$, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.   \n* Notice that BIC replaces the $2d\\hat\\sigma^2$ used by $C_{p}$ with a $log(n)d\\hat\\sigma^2$ term, where $n$ is the number of observations.    \n* Since $log n > 2$ for any $n>7$, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than $C_{p}$.     \n\n###Adjusted $R^2$    \n* For a least squares model with $d$ variables, the adjusted $R^2$ statistic is calculated as    \nAdjusted $R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}$.   \nWhere TSS is the total sum of squares.    \n* Unlike $C_{p}$, AIC, and BIC, for which a *small* value indicates a model with a low test error, a *large* vlaue of adjusted $R^2$ indicates a model with a small test error.    \n* Maximizing the adjusted $R^2$ is equivalent to minimizing $\\frac{RSS}{n-d-1}. While RSS always decreases as the number of variables in the model increases, \\frac{RSS}{n-d-1} may increase or decrease, due to the presence of d in the denominator.    \n* Unlike the $R^2$ statistic, the adjusted $R^2$ statistic *pays a price* for the inclusion of unnecessary variables in the model..    \n\n##Validation and Cross-Validation   \n***    \n* Each of the procedures returns a sequence of models $M_{k}$ indexed by model size $k=0,1,2,...$ Our job here is to select $\\hat k$. Once selected, we will return model $M_{\\hat k}$.   \n* We compute the validation set error or the cross-validation error for each model $M_{k}$ under consideration, and then select the $k$ for which the resulting estimated test error is smallest.   \n* This procedure has an advantage relatibe to AIC, BIC,$C_{p}$, and adjusted $R^2$, in that it provdies a direct estimate of the test error, and *doesn't require an estimate of the error variance $\\simga^2$*.   \n* It can also be used in a wider range of model selection taks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance $\\simga^2$.    \n\n* *One-standard-error-rule*. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. *What is the rational for this?*    \n\n##Shrinkage methods and ridge regression   \n***    \n###Shrinkage Methods    \n***Ridge Regression*** and ***Lasso***   \n* The subset selection method use least squares to fit a linear model that contains a subset of the predictors.    \n*  As an alternative, we can fit a model containing all p predictors using a technique that *constrains* or *regularizes* the coefficient estimates, or equivalently, that *shrinks* and coefficient estimates towards zero.    \n* It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.     \n\n###Ridge regression   \n* Recall that the least squares fitting procedures estimates $\\beta_{0},\\beta_{1},..,\\beta_{P}$ using the values that minimize     \n$$ RSS=\\sum_{i=1}^n \\Big(y_{i}-\\beta_{0}-\\sum_{j=1}^p \\beta_{j}x_{ij} \\Big)^2 $$.    \n* In contrast, the ridge regression coefficient estimates $\\hat\\beta^R$ are the values that minimize      \n$$ \\sum_{i=1}^n \\Big( y_{i} - \\beta_{0} - \\sum_{j=1}^p \\beta_{j}x_{ij}\\Big)^2 + \\lambda\\sum_{j=1}^p \\beta_{j}^2 = RSS + \\lambda\\sum_{j=1}^p \\beta_{j}^2 $$    \nWhere $\\lambda \\geq 0$ is a *turning parameter*, to be determined seperately.     \n\n###Ridge regression: continued    \n* As with least squares, ridge regression seeks coefficient estimates the fit the data well, by making the RSS small.    \n* However, the second term, $\\lambda\\sum_{j} \\beta_{j}^2$, called a *shrinkage penalty*, is small when $\\beta_{1},...,\\beta_{p}$ are close to zero, and so it has the effect of *shrinking* the estimates of $\\beta_{j}$ towards zero.    \n* The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates.     \n* Selecing a good value for $\\lambda$ is critical; cross-validation is used for this.     \n\n###Ridge regression: scaling of predictors    \n* The standard least squares coefficient estimates are *scale equivariant*: multiplying $X_{j}$ by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of $\\frac{1}{c}$. In other wordsm regardless of how the $j^th$ predictor is scaled, $X_{j}\\hat\\beta_{j}$ will remain the same.    \n* In contrast, the ridge regression coefficient estimates can change *substantially* when mutliplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.   \n* Therefore, it is best to apply ridge regression after *standardizing the predictors*, using the formula    \n$$ \\tilde x_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{ij}-\\bar x_{j})^2}}\n $$   \n \n## The Lasso\n***    \n* Ridge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model   \n* The *Lasso* is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coeffficients, $\\hat\\beta_{\\lambda}^L$, minimize the quantity     \n$$ \\sum_{i=1}^n \\Big( y_{i}-\\beta_{0}-\\sum_{j=1}^p \\beta_{j}x_{ij} \\Big)^2 + \\lambda\\sum_{j=1}^p |\\beta_{j}| = RSS + \\lambda\\sum_{j=1}^p |\\beta_{j}| $$    \n* In statistical parlance, the lasso uses an $L_{1}$ norm of a coefficient vector $\\beta$ is given by $||\\beta||=\\sum|\\beta_{j}|$.    \n\n###The Lasso: continued   \n* As with ridge regression, the lasso shrinks the coefficient estimates towards zero.    \n* However, in the case of the lasso, the $L_{1}$ penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter $\\lambda$ is sufficiently large.    \n* Hence, much like best subset selection, the lasso performs *variable selection*.    \n* We say that the lasso yields *sparse* models-that is, models that involve only a subset of the variables.    \n* As in ridge regression, selection a good value of $\\lambda$ for the lasso is critical; cross-validation is again the method of choice.    \n\n###The Variable Selection Property of the Lasso    \nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?    \nOne can show that the lasso and ridge regression coefficient estimates solve the problems    \n$$ \\underset{x}{\\text{minimize}} = \\sum_{i=1}^n \\Big( y_{i}-\\beta_{0}-\\sum_{j=1}^p \\beta_{j}x_{ij} \\Big)^2 \\text{subject to} \\sum_{j=1}^p |\\beta_{j}|$$    \nand     \n$$ \\underset{x}{\\text{minimize}} = \\sum_{i=1}^n \\Big( y_{i}-\\beta_{0}-\\sum_{j=1}^p \\beta_{j}x_{ij} \\Big)^2 \\text{subject to} \\sum_{j=1}^p \\beta_{j}^2 \\leq s $$    \nrespectively.     \n\n###Conclusions   \n* Neither ridge regression nor the lasso will universally dominate the other.    \n* In general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.   \n* However, the number of predictors that is related to the response is never known *a priori* for real data sets.    \n* A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.     \n\n##Tuning parameter selection   \n***    \n###Selection the Tuning Parameter for Ridge Regression and Lasso     \n* As for subset selection, for ridge regressio and lasso we require a method to determine which of the models under consideration is best.    \n* That is, we require a method selecting a value for the tuning parameter $\\lambda$ or equivalently, the value of the constraint s.   \n* *Cross-validation* provides a simple way to tackle this problem. We choose a grid of $\\lambda$ values, and compute the cross-validation error rate for each value of $\\lambda$.    \n* We then select the tuning parameter value for which the cross-validation error is smallest.    \n* Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.     \n\n##Dimension Reduction Methods    \n***    \n* The methods that we have discussed so far in this chapter have involved fitting linear regression models, via leaset squares or shrunken approach, using the original predictors, $X_{1},X_{2},...,X_{p}$.    \n* We now explore a class of approaches that *transform* the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as *dimension reduction* methods.     \n\n###Dimension Reduction Methods: details    \n* Let $Z_{1},Z_{2},...,Z_{M}$ represent $M < p$ *linear combinations of our original $p$ predictors. That is,      \n$$ Z_{m}=\\sum_{j=1}^p \\phi_{mj}X_{j} $$    \nfor some contants $\\phi_{m1},...,\\phi_{mp}$.     \n* We can then fit the linear regression model     \n$$ y_{i} = \\theta_{0} + \\sum_{m=1}^M \\theta_{m}z_{im} + \\epsilon_{i} $$,\n$i = 1,...,n$,     \nUsing ordinary least squares.    \n* Note that in model (2), the regression coefficients are given by $\\theta_{0},\\theta_{1},...,\\theta_{M}$. If the constants $\\phi_{m1},...,\\phi_{mp}$ are chosen wisely, then such dimension reduction approaches can often outperform OLS regression.      \n* Notice that from definition (1),   \n$$ \\sum_{m=1}^M \\theta_{m}z_{im} = \\sum_{m=1}^M \\theta_{m} \\sum_{j=1}^p \\phi_{mj}x_{ij}=\\sum_{j=1}^P\\sum_{m=1}^M \\theta_{m}\\phi_{mj}x_{ij} = \\sum_{j=1}^p\\beta_{j}x_{ij} $$,  \nwhere     \n$$ \\beta_{j}=\\sum_{m=1}^M\\beta_{m}\\phi_{mj} $$.    \n* Hence model (2) can be though of as a special case of the original linear regression model.    \n* Dimension reduction serves to constrain the estimated $\\beta_{j}$ coefficients, since now they must take the form (3).     \n* Can win in the bias-variance tradeoff.     \n\n##Principal Components Regression and Partial Least Squares    \n***   \n###Principal Components Regression    \n* Here we apply principal components analysis (PCA) to define the linear combinations of the predictors, for use in our regression.    \n* The first principal component is that (normalized) linear combination of the variables. with the largest variance.\n* The second principla component has largest variance, subject to being uncorrelated with the first.    \n* And so on.   \n* Hence with many correlated original variables, we replace them with a small set of principle components that capture their joint variation.    \n\n###Partial Least Squares   \n* PCR identifies linear combinations, or *directions*, that best represent the predictors $X_{1},...,X_{p}$.    \n* These directions are identified in an *unsupervised* way, since the response $Y$ is not used to help determine the principal component directions.   \n* That is, the response does not *supervise* the identification of the principal components.    \n* Consequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.    \n* Like PCR, PLS is a dimension reduction method, which first identifies a new set of features $Z_{1},...,Z_{M}$ that are linear combinations of the orginial features, and then fits a linear model via OLS using these M new features.    \n* But unlike PCR, PLS identifies these new features in a supervised way - that is, it makes use of teh response Y in order to identify new features that not only approximate the old features well, but also that *are related to the response*.    \n* Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.  \n\n###Details of Partial Least Squares   \n* After standardizing the $p$ predictors, PLS computes the first direction  $Z_{1}$ by setting each $\\phi_{1j}$ in (1) equal to the coefficient from the simple linear regression of $Y$ onto $X_{j}$.     \n* One can show that this coefficient is proportional to the correlation between $Y$ and $X_{j}$.     \n* Hence, in computing $Z_{1}=\\sum_{j=1}^p \\phi_{1j}X_{j}$, PLS places the highest weight on the variables that are most strongly related to the response.   \n* Subsequent directions are found by taking residuals and then repeated the above prescription.      \n\n###Summary   \n* Model selection methods are an essential tool for data analysis, especially for big datasets involving many predictors.    \n* Research into methods that give *sparsity*, such as the *lasso* is an especially hot area.   \n* Later, we will return to sparsity in more detail, and will describe related approaches such as the *elastic net*.     \n\n##Model Selection in R    \n================================  \n```{r}\nlibrary(ISLR)\nsummary(Hitters)\n```\nThere are some missing values here, so before we proceed we will remove them:\n\n```{r}\nHitters=na.omit(Hitters)\nwith(Hitters, sum(is.na(Salary)))\n```\n\n\nBest Subset regression\n------------------------\nwe will now use the package 'leaps' to evaluate all the best-subset models.    \n```{r}\n#install.packages(\"leaps\")\nlibrary(leaps)\nregfit.full = regsubsets(Salary~.,data=Hitters)\nsummary(regfit.full)\n```\nIt gives by default best-subsets up to size 8; lets increase that to 19, i.e. all the variables     \n```{r}\nregfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)\nreg.summary =  summary(regfit.full)\nnames(reg.summary)\nplot(reg.summary$cp, xlab=\"number of Variables\", ylab=\"Cp\")\nwhich.min(reg.summary$cp)\npoints(10,reg.summary$cp[10],pch=20,col=\"red\")\n```\nThere is a plot method for the 'regsubsets' object    \n```{r}\nplot(regfit.full,scale=\"Cp\")\ncoef(regfit.full,10)\n```\n\nForward Stepwise Selection\n-----------------------------\nHere we use the 'regsubsets' function but specify the 'method = \"forward\"' option:     \n```{r}\nregfit.fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = \"forward\")  \nsummary(regfit.fwd)   \nplot(regfit.fwd, scale = \"Cp\")   \n```\n\n\nModel Selection Using a Validation Set    \n---------------------------------------\nLets make a training and validation set, so that we can choose a good subset model.      \nWe will do it using a slightly different approach from what was done in the book.   \n```{r}\ndim(Hitters)\nset.seed(1)\ntrain = sample(seq(263),180,replace=FALSE)\ntrain\nregfit.fwd = regsubsets(Salary~.,data=Hitters[train,], nvmax=19, method = \"forward\")\n```\nNow we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. We have to do a bit of work here, because there is no predict method for 'regsubsets'.     \n```{r}\nval.errors=rep(NA,19)\nx.test=model.matrix(Salary~., data=Hitters[-train,]) # notice the index!    \nfor(i in 1:19){\n    coefi = coef(regfit.fwd, id = i)\n    pred = x.test[,names(coefi)]%*%coefi\n    val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)\n}\nplot(sqrt(val.errors), ylab=\"Root MSE\", ylim=c(300,400), pch=19, type=\"b\")\npoints(sqrt(regfit.fwd$rss[-1]/180), col=\"blue\", pch=19,type=\"b\")\nlegend(\"topright\", legend=c(\"Training\", \"Validation\"), col=c(\"blue\",\"black\"),pch=19)\n```\nAs we expect, the training error goes down monotonically as the model gets bigger, but not so for the validation error.   \n\nThis was a little tedious - not having a predict method for 'regsubsets'.    \nSo we will write one!     \n```{r}\npredict.regsubsets=function(object, newdata,id,...){\n    form=as.formula(object$call[[2]])\n    mat = model.matrix(form, newdata)\n    coefi=coef(object,id=id)\n    mat[,names(coefi)]%*%coefi\n}\n```\n\n\n\nModel Selection by Cross-Validation\n-------------------------------------\nWe will do 10-fold cross-validation. Its really easy!   \n```{r}\nset.seed(11)\nfolds = sample(rep(1:10, length=nrow(Hitters)))\nfolds\ntable(folds)\ncv.errors = matrix(NA,10,19) # make matrix for our errors\nfor(k in 1:10){\n    best.fit=regsubsets(Salary~., data=Hitters[folds!=k,], nvmax=19, method=\"forward\")\n    for(i in 1:19){\n        pred = predict(best.fit, Hitters[folds==k,], id=i)\n        cv.errors[k,i] = mean( (Hitters$Salary[folds==k] - pred)^2)\n    }\n}\nrmse.cv = sqrt(apply(cv.errors,2,mean))\nplot(rmse.cv,pch=19,type=\"b\")\n```\n\nRidge Regression and the Lasso \n-------------------------------\nWe will use the package `glment`, which does not use the model formula language, so we will set up an `x` and `y`.    \n```{r}\n#install.packages(\"glmnet\", repos = \"http://cran.us.r-project.org\")\nlibrary(glmnet)\nx = model.matrix(Salary~., data = Hitters)\ny = Hitters$Salary \n```\nFirst we will fit a ridge-regression model. This is achieved by calling `glmnet` with `alpha=0` (see the helpfile). There is also a `cv.glmnet` function which will do the cross-validation for us.    \n```{r}\nfit.ridge = glmnet(x,y,alpha=0) # 0 ridge 1 lasso. between are elastic net\nplot(fit.ridge,xvar=\"lambda\", label=TRUE)\ncv.ridge=cv.glmnet(x,y,alpha=0)\nplot(cv.ridge)\n```\nNow we fit a lasso model; for this we use the default `alpha=1`\n```{r}\nfit.lasso=glmnet(x,y)\nplot(fit.lasso, xvar=\"lambda\", label=TRUE)\ncv.lasso=cv.glmnet(x,y)\nplot(cv.lasso)\ncoef(cv.lasso)\n\n```\n\nSuppose we want to use our earlier train/validation division to select the `lambda` for the lasso.   \nThis is easy to do      \n```{r}\nlasso.tr=glmnet(x[train,],y[train])\nlasso.tr\npred=predict(lasso.tr,x[-train,])\ndim(pred)\nrmse = sqrt(apply((y[-train]-pred)^2,2,mean)) # y is broadcast to the dimension of pred\nplot(log(lasso.tr$lambda), rmse, type=\"b\", xlab=\"Log(lambda)\")\nlam.best = lasso.tr$lambda[order(rmse)[1]]\nlam.best\ncoef(lasso.tr, s = lam.best)\n```\n\n#Chapter 7 Moving Beyond Linearity  \n***     \n##Polynomial and Step Functions\n***       \n**Moving Beyond Linearity**           \n*The truth is never linear!*       \n*Or almost never!*      \n\nBut often the linearity assumption is good enough.   \n\nWhen its not...        \n* polynomials,     \n* step functions,       \n* splines,        \n* local regression, and      \n* generalized additive models      \n\noffer a lot of flexibility, without losing the ease and interpretability of linear models.     \n\n###Polynomial Regression     \n\n$y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}x_{i}^2+\\beta_{3}x_{i}^3+...+\\beta_{d}x_{i}^d+\\epsilon_{i}$       \n\n###Details     \n* Create new variables  $X_{1} = X,X_{2}=X^2$, etc. and then treat as multiple linear regression.    \n* Not really interested in the coefficients; more interested in the fitted function values at any value $x_{0}$:     \n$$ \\hat f(x_{0})=\\hat\\beta_{0}+\\hat\\beta_{2}^2+\\hat\\beta_{3}^3+\\hat\\beta_{4}^4 $$     \n\n* Since $\\hat f(x_{0})$ is a linear function of $\\hat\\beta_{\\ell}$, can get a simple expression for *pointwise-variance* $Var[\\hat f(x_{0})]$ at any value $x_{0}$. In the figure we have computed the fit and pointwise standard errors on a grid of values for $x_{0}$. We show $\\hat f(x_{0}) \\pm 2 \\cdot se[\\hat f(x_{0})]$.       \n* We either fix the degree $d$ at some reasonably low value, else use cross-validation to choose $d$.     \n\n\n###Details continued     \n* Logistic regression follows naturally. For example, in figure we model    \n$$ Pr(y_{i} > 250\\mid x_{i}) = \\frac{exp(\\beta_{0}+\\beta_{2}x_{i}^2 +...+\\beta_{d}x_{i}^d)}{1 + exp(\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}x_{i}^2+..+\\beta_{d}x_{i}^d)} $$     \n* To get confidence intervals, compute upper and lower bounds on *the logit scale*, and then invert to get on probability scale.       \n* Can do seperately on several variables--just stack the variables into one matrix, and seperate out the pieces afterwards (see GAMs later).     \n* Caveat: polynomials have notorious tail behavior - very bad for extrapolation.      \n* Can fit using ```{r}y ~ poly(x,degree = 3)``` in formula.       \n\n###Step Functions      \nAnother way of creating transformations of a variable- cut the variable into distinct regions    \n$$ C_{1}(X)=I(X<35), C_{2}(X)=I(35\\leq X < 50),...,C_{3}(X)=I(X \\geq 65) $$   \n* Easy to work with. Creates a series of dummy variables representing each group.    \n* Useful way of creating interactions that are easy to interpret. For example, interaction effect of **Year** and **Age**:     \n$$ I(\\text{Year}<2005) \\cdot \\text{Age}, I(\\text{Year} \\geq 2005) \\cdot \\text{Age}} $$      \nwould allow for different linear functions in each age category.     \n* In R:```{r}I(year < 2005)``` or ```{r}cut(age,c(18,25,40,65,90))```.     \n* Choice of cutpoints or *Knots* can be problematic. For creating nonlinearities, smoother alternatives such as *splines* are available.       \n\n##Peicewise-Polynomials and Splines     \n***    \n###Piecewise Polynomials       \n* Instead of a single polynomial in X over its whole domain, we can rather use different polynomials in regions defined by knots. E.g. (see figure)    \n$$ y_{i} = \\begin{cases} \\beta_{01} + \\beta_{11}x_{i}+\\beta_{21}x_{i}^2 +\\beta_{31}x_{i}^2 + \\epsilon_{i}, &\\text{if} x_{i} < c; \\\\ \\beta_{02} + \\beta_{12}x_{i} + \\beta_{22}x_{i}^2+\\beta_{32}x_{i}^3 + \\epsilon_{i}, &\\text{if}x_{i} \\geq c.  \\end{cases}  $$      \n\n* Better to add contraints to the polynomials, e.g. continuity.     \n* *Splines* have the \"maximum\" amount of continuity.      \n\n###Linear Splines    \n*A linear spline with knots at $\\xi, k=1,...K$ is a piecewise linear polynomial continuous at each knot*.     \nwe can represent this model as     \n$$ y_{i} = \\beta_{0} + \\beta_{1}b_{1}(x_{i})+\\beta_{2}b_{2}(x_{i})+...+\\beta_{K+3}b_{K+3}(x_{i}) + \\epsilon_{i} $$     \nwhere the $b_{k}$ are *basis functions.*        \n$b_{1}(x_{i}) = x_{i}$     \n$b_{k+1}(x_{i}) = (x_{i}-\\xi_{k})_{+}, k =1,...,K$      \nHere the $()_{+}$ means *positive part*; i.e.     \n\n$$ (x_{i} - \\xi_{k}) = \\begin{cases} x_{i}-\\xi_{k} &\\text{if} x_{i} > \\xi_{k} \\\\ 0 &\\text{otherwise} \\end{cases} $$     \n\n###Cubic Splines     \n*A cubic spline with knots at $\\xi_{k},k=1,...,K$ is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.*    \nAgain we can represent this model with truncated power basis functions      \n$y_{i}=\\beta_{0}_\\beta_{1}b_{1}(x_{i}) + \\beta_{2}b_{2}(x_{i})+...+\\beta_{K+3}b_{K+3}(x_{i}) + \\epsilon_{i}$,    \n$b_{1}(x_{i})=x_{i}$      \n$b_{2}(x_{i})=x_{i}^2$      \n$b_{3}(x_{i})=x_{i}^3$\n$b_{K+3}(x_{i}) = (x_{i}-\\xi_{k})_{+}^3, k=1,..,K$     \nwhere      \n$$ (x_{i} - \\xi_{k})_{+}^3 = \\begin{cases} (x_{i}-\\xi_{k})^3 &\\text{if} x_{i} > \\xi_{k} \\\\ 0 &\\text{otherwise} \\end{cases} $$  \n\n###Natural Cubic Splines     \nA natural cubic spline extrapolates linearly beyond the boundary knots. This adds $4=2\\times 2$ extra contraints, and allow us to put more internal knots for the same degrees of freedom as a regular cubic spline.      \n\nFitting splines in R is easy: ```{r} bs(x,...)``` for any degree splines, and ```{r}ns(x,...)``` for natural cubic splines, in package ```{r}splines```.     \n\n###Knot placement    \n* One strategy is to decide K, the number of knots, and then place them at appropriate quantiles of the observed X.    \n* A cubic spline with K knots has K + 4 parameters or degrees of freedom. \n* A natural spline with K knots has K degress of freedom.    \n\n##Smoothing Splines    \n***     \nThis section is a little bit mathematical!      \nConsider this criterion for fitting a smooth function $g(x)$ to some data:     \n$$ \\underset{g \\in S}{\\text{minimize}}\\sum_{i=1}^n (y_{i}-g(x_{i}))^2 + \\lambda \\int g''(t)^2dt $$      \n\n* The first term is RSS, and tries to make $g(x)$ match the data at each $x_{i}$.     \n* The second term is a *roughness penalty* and controls how wiggly $g(x)$ is. It is modulated by the *tuning parameter* $\\lambda \\geq 0$.     \n    * The smaller $\\lambda$, the more wiggly the function, eventually interpolating $y_{i}$ when $\\lamda = 0$.        \n    * As $\\lambda \\rightarrow \\infty$, the function becomes linear\n\nThe solution is a natural cubic spline, with a knot at every unique value of $x_{i}$. The roughness penalty still controls the roughness via $\\lambda$.    \nSome details    \n    * Smoothing splines avoid the knot-selection issue, leaving a single $\\lambda$ to be chosen.    \n    * The algorithmic details are too complex to describe here. In R, the function ```{r}smooth.spline()``` will fit a smoothing spline.      \n    * The vector of $n$ fitted values can be written as $\\hat g_{\\lambd}=S_{\\lambda}y$ where $S_{\\lambda}$ is a $n \\times n$ matrix (determined by the $x_{i}$ and $\\lambda$).   \n    * The *effective degrees of freedom* are given by       \n    $$df_{lambda}=\\sum_{i=1}^n{S_{\\lambda}}_{ii} $$     \n    \n* We can specify $df$ rather than $\\lambda$!\n    * In R: ```{r}smooth.spline(age,wage,df=10)```    \n* The leave-one-out (LOO) cross-validated error is given by    \n$$ RSS_{cv}(\\lambda)=\\sum_{i=1}^n(y_{i}-\\hatg_{\\lambda}^{-i}(x_{i}))^2=\\sum_{i=1}^n \\Big[\\frac{y_{i}-\\hatg_{\\lambda}(x_{i})}{1 - {S_{\\lambda}}_{ii}}\\Big]^2  $$        \n    * In R:```{r}smooth.spline(age,wage)```      \n\n##Generalized Additive Models and Local Regression     \n###Local Regression      \nWith a sliding weight function, we fit seperate linear fits over teh range of $X$ by weighted least squares.     \n\n###Generalized Additive Models     \nAllows for flexile nonlinearities in several variables, but retains the additive structure of linear models.     \n$$ y_{i} = \\beta_{0} + f_{1}(x_{i1}) + f_{2}(x_{i2})+...+f_{p}(x_{ip}) + \\epsilom_{i} $$     \n\n###GAM details     \n* Can fit a GAM simply using, e.g. natural splines:    \n```{r}lm(wage ~ ns(year,df=5) + ns(age, df=5)+education)```      \n* Coefficients not that interesting; fitted functions are. The previous plot was produced using ```{r}plot.gam```.       \n* Can mix terms - some linear, some nonlinear - and use ```{r}anova()``` to compare models.     \n* Can use smoothing splines or local regression as well:     \n```{r}gam(wage~s(year,df=5)+lo(age,span=.5)+education)```     \n* GAMs are additive. although low-order interactions can be included in a natural way using, e.g. bivariate smoothers or interactions of the form ```{r}ns(age,df=5):ns(year,df=5)```.      \n\n###GAMs for classification     \n$$ log \\big(\\frac{p(X)}{1-p(X)}\\big) = beta_{0} + f_{1}(X_{1}) + f_{2}(X_{2})+...+f_{p}(X_{p}) $$     \n\n##Nonlinear Functions in R\n***\nNonlinear Models\n========================================\nhere we explore the use of nonlinear models using some tools in R\n```{r}\nrequire(ISLR)\nattach(Wage)\n```\n\nPolynomials\n--------------\n\nFirst we will use polynomials, and focus on a single predictor age:\n\n```{r}\nfit = lm(wage~poly(age,4),data=Wage) \nsummary(fit)\n```\n\nThe `poly()` functions generates a basis of *orthogonal polynomials*.    \nLet's make a plot of the fitted function, along with the standard errors of the fit.     \n\n```{r fig.width=7, fig.height=6}\nagelims=range(age)\nage.grid = seq(from=agelims[1], to=agelims[2])\npreds = predict(fit,newdata=list(age=age.grid), se=TRUE)\nse.bands = cbind(preds$fit+2*preds$se,preds$fit-2*preds$se)\nplot(age,wage,col='darkgrey')\nlines(age.grid,preds$fit, lwd=2, col=\"blue\")\nmatlines(age.grid, se.bands, col=\"blue\",lty=2)\n```\n\n___\n# Examples: \n\nInstall and load necessary datasets and functions\n```{r}\nlibrary(\"MASS\")\nlibrary(\"ISLR\")\nlibrary(\"ggplot2\")\nlibrary(\"car\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\n```\n\n## 3.6.2 Simple Linear Regression    \n\n*The MASS library contains the Boston data set, which records medv (median house value) for 506 neighborhoods around Boston. We will seek to predict medv using 13 predictors such as rm (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status).*  \n```{r}\n\n#View(Boston)\nnames(Boston)\n#?Boston\n```\n\n*We will start by using the lm() function to fit a simple linear regression\nmodel, with medv as the response and lstat as the predictor. The basic lm() syntax is lm(y∼x,data), where y is the response, x is the predictor, and\ndata is the data set in which these two variables are kept.*     \n```{r}\nlm.fit = lm(medv~lstat, data = Boston)\nlm.fit\nsummary(lm.fit)\n```\n\n*We can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quan- tities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.*        \n```{r}\nnames(lm.fit)\ncoef(lm.fit)\n```\n\n\n*In order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.*     \n```{r}\nconfint(lm.fit)\n```\n*The predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.*    \n```{r}\npredict(lm.fit, data.frame(lstat=(c(5,10,15))), interval = \"confidence\")\npredict(lm.fit, data.frame(lstat=(c(5,10,15))), interval = \"prediction\")\n```\n*We will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.*      \n```{r, echo=FALSE}\nggplot(data = Boston, aes( x= lstat, y = medv)) +\n    geom_point(pch = \"+\") +\n    geom_smooth(method = \"lm\")\n```\n    \n*Next we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by ap- plying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() function, which tells R to split the display screen into separate panels so that multiple plots can be viewed si- multaneously. For example, par(mfrow=c(2,2)) divides the plotting region into a 2 × 2 grid of panels.*      \n```{r}\npar(mfrow=c(2,2))\nplot(lm.fit)\n```\n    \n*Alternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.*      \n```{r, echo=FALSE}\nplot(predict(lm.fit), residuals(lm.fit)) \nplot(predict(lm.fit), rstudent(lm.fit))\n```\n    \n*On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.*       \n```{r,echo=FALSE}\nplot(hatvalues (lm.fit))\nwhich.max(hatvalues (lm.fit))\n```\n___\n\n## 3.6.3 Multiple Linear Regression   \n*In order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y∼x1+x2+x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.*        \n```{r}\nlm.fit=lm(medv ~ lstat+age,data=Boston) \nsummary(lm.fit)\n```\n*The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:*      \n```{r}\nlm.fit=lm(medv ~., data=Boston)\nsummary(lm.fit)\n```\n*We can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the R2, and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages option in R.*       ￼￼￼￼￼￼￼\n```{r}\n?summary.lm\nsummary(lm.fit)$r.sq\nsummary(lm.fit)$sigma\nvif(lm.fit)\n```\n*What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.*        \n```{r}\nlm.fit1=lm(medv~.-age,data=Boston) \nsummary(lm.fit1)\n```\n*Alternatively, the update() function can be used.*        \n```{r}\n#￼lm.fit1= update(lm.fit , ~. -age)\n\n```\n\n___\n\n## 3.6.4 Interaction Terms     \n*It is easy to include interaction terms in a linear model using the lm() func- tion. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax ```lstat*age``` simultaneously includes lstat, age, and the interaction term lstat×age as predictors; it is a shorthand for ```lstat+age+lstat:age```.*      \n```{r}\nsummary(lm(medv ~ lstat*age, data = Boston))\n```\n\n___\n\n## 3.6.5 Non-linear Transformations of the Predictors   \n   \n*The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor X, we can create a predictor X2 using ```I(X^2)```. The function I() is needed since the ^ has a special meaning\nin a formula; wrapping as we do allows the standard usage in R, which is I() to raise X to the power 2. We now perform a regression of medv onto lstat and lstat2.*    \n```{r}\nlm.fit2 = lm(medv~ lstat + I(lstat^2), data = Boston)\nsummary(lm.fit2)\n```\n      \n*The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.*    \n```{r}\nlm.fit <- lm(medv ~ lstat, data = Boston)\nanova(lm.fit, lm.fit2)\n```\n   \n*Here Model 1 represents the linear submodel containing only one predictor,\nlstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type* \n\n```{r}\npar(mfrow = c(2,2))\nplot(lm.fit2)\n```\n\n*In order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher- order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:* \n```{r}\nlm.fit5 = lm(medv ~ poly (lstat, 5), data = Boston)\nsummary(lm.fit5)\n```\n\n*Of course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.*   \n```{r}\nsummary(lm(medv ~ log(rm), data = Boston))\n```\n\n___\n## 3.6.6 Qualitative Predictors  \n*We will now examine the Carseats data, which is part of the ISLR library. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.*   \n```{r}\n#fix(Carseats)\n#View(Carseats)\nnames(Carseats)\n```\n*The Carseats data includes qualitative predictors such as Shelveloc, an in- dicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The pre- dictor Shelveloc takes on three possible values, Bad, Medium, and Good.*   \n*Given a qualitative variable such as Shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.*   \n```{r}\nlm.fit = lm(Sales~.+Income:Advertising+Price:Age, data = Carseats)\nsummary(lm.fit)\n```\n*The contrasts() function returns the coding that R uses for the dummy variables.*   \n```{r}\nattach(Carseats)\ncontrasts(ShelveLoc)\n?contrasts\n```\n*R has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location but lower sales than a good shelving location.*   \n***\n**3.6.7 Writing Functions**   \n*As we have seen, R comes with many useful functions, and still more func- tions are available by way of R libraries. However, we will often be inter- ested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.*  \n```{r}\nLoadLibraries = function(){\n    library(ISLR)\n    library(MASS)\n    print (\"The libraries have been loaded.\")\n}\n```\n*Now if we type in LoadLibraries, R will tell us what is in the function.\n*  \n```{r}\nLoadLibraries\n```\n*If we call the function, the libraries are loaded in and the print statement is output.*  \n```{r}\nLoadLibraries()\n\n```\n***\n**3.7 Exercises**  \n*CONCEPTUAL*   \n    1. Describe the null hypothesis to which the p-values given in tables 3.4 correspond. Explain what conclusion you can draw on these p-values. your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of coefficients of the linear model.\n*Answer*:__insert_here__    \n    2. Carefully explain the differences between the KNN classifier and KNN regression methods.      \n*Answer*:__insert_here__       \n    3. Suppose we have a data set with five predictions,$X_1=GPA$,$X_2=IQ$,$X_3=Gender$, and $X_5=Interaction$ between GPA and Gender.\n\n\n\n\n\n\n\n\n",
    "created" : 1437684594042.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1152042062",
    "id" : "6B3949F2",
    "lastKnownWriteTime" : 1443576400,
    "path" : "~/Desktop/Statistical_learning(MachineLearning)_Resources/Intro_Statistical_learning_rprog/Intro_StatLearn_Examples.Rmd",
    "project_path" : "Intro_StatLearn_Examples.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}